import asyncio
import aiohttp
import ssl
import certifi
import logging
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, CallbackQueryHandler, MessageHandler, filters, ContextTypes
from bs4 import BeautifulSoup
import re
import json
from urllib.parse import urljoin, urlparse

# áƒšáƒáƒ’áƒ˜áƒœáƒ’áƒ˜áƒ¡ áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ£áƒ áƒáƒªáƒ˜áƒ
logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

class ProductBot:
    def __init__(self, bot_token):
        self.bot_token = bot_token
        self.session = None
        
    async def init_session(self):
        """HTTP áƒ¡áƒ”áƒ¡áƒ˜áƒ˜áƒ¡ áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ SSL áƒ¡áƒ”áƒ áƒ¢áƒ˜áƒ¤áƒ˜áƒ™áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ—"""
        # SSL áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ¥áƒ›áƒœáƒ
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        ssl_context.check_hostname = False  # áƒ–áƒáƒ’áƒ˜áƒ”áƒ áƒ—áƒ˜ áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡
        ssl_context.verify_mode = ssl.CERT_REQUIRED
        
        # HTTP áƒ™áƒáƒœáƒ”áƒ¥áƒ¢áƒáƒ áƒ˜ SSL-áƒ˜áƒ—
        connector = aiohttp.TCPConnector(
            ssl=ssl_context,
            limit=100,
            limit_per_host=10,
            ttl_dns_cache=300,
            use_dns_cache=True,
            keepalive_timeout=60,
            enable_cleanup_closed=True
        )
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=aiohttp.ClientTimeout(total=30, connect=10),
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'ka-GE,ka;q=0.9,en;q=0.8,ru;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',  # brotli áƒ›áƒ®áƒáƒ áƒ“áƒáƒ­áƒ”áƒ áƒ
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0'
            }
        )
    
    async def close_session(self):
        """áƒ¡áƒ”áƒ¡áƒ˜áƒ˜áƒ¡ áƒ“áƒáƒ®áƒ£áƒ áƒ•áƒ"""
        if self.session:
            await self.session.close()
    
    async def fetch_website_content(self, url):
        """áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ“áƒáƒœ áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒ›áƒáƒáƒáƒ•áƒ”áƒ‘áƒ SSL áƒ›áƒ®áƒáƒ áƒ“áƒáƒ­áƒ”áƒ áƒ˜áƒ—"""
        try:
            if not self.session:
                await self.init_session()
            
            # URL-áƒ˜áƒ¡ áƒœáƒáƒ áƒ›áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ
            if not url.startswith(('http://', 'https://')):
                url = 'https://' + url
            
            # Brotli compression áƒ›áƒ®áƒáƒ áƒ“áƒáƒ­áƒ”áƒ áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡
            headers = {
                'Accept-Encoding': 'gzip, deflate, br',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache'
            }
            
            # áƒáƒ˜áƒ áƒ•áƒ”áƒš áƒ áƒ˜áƒ’áƒ¨áƒ˜ HTTPS-áƒ˜áƒ¡ áƒ›áƒªáƒ“áƒ”áƒšáƒáƒ‘áƒ
            if url.startswith('http://'):
                https_url = url.replace('http://', 'https://', 1)
                try:
                    async with self.session.get(https_url, headers=headers) as response:
                        if response.status == 200:
                            content = await response.text(encoding='utf-8', errors='ignore')
                            return content
                except Exception as e:
                    logger.warning(f"HTTPS failed for {https_url}: {e}, trying HTTP...")
            
            # áƒ«áƒ˜áƒ áƒ˜áƒ—áƒáƒ“áƒ˜ áƒ›áƒªáƒ“áƒ”áƒšáƒáƒ‘áƒ
            try:
                async with self.session.get(url, headers=headers) as response:
                    if response.status == 200:
                        content = await response.text(encoding='utf-8', errors='ignore')
                        return content
                    elif response.status in [301, 302, 303, 307, 308]:
                        # Redirect handling
                        redirect_url = str(response.url)
                        logger.info(f"Redirected to: {redirect_url}")
                        async with self.session.get(redirect_url, headers=headers) as redirect_response:
                            if redirect_response.status == 200:
                                content = await redirect_response.text(encoding='utf-8', errors='ignore')
                                return content
                    else:
                        logger.error(f"HTTP Error {response.status} for URL: {url}")
                        return None
                        
            except aiohttp.ContentTypeError as cte_error:
                logger.warning(f"Content-Type error for {url}, trying without encoding: {cte_error}")
                # Content-Type error-áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒ—áƒ®áƒ•áƒ”áƒ•áƒáƒ¨áƒ˜ encoding-áƒ˜áƒ¡ áƒ’áƒáƒ áƒ”áƒ¨áƒ”
                async with self.session.get(url, headers={'Accept-Encoding': 'identity'}) as response:
                    if response.status == 200:
                        content = await response.read()
                        return content.decode('utf-8', errors='ignore')
                    
        except aiohttp.ClientSSLError as ssl_error:
            logger.error(f"SSL Error for {url}: {ssl_error}")
            # SSL áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒ—áƒ®áƒ•áƒ”áƒ•áƒáƒ¨áƒ˜ HTTP-áƒ˜áƒ¡ áƒ›áƒªáƒ“áƒ”áƒšáƒáƒ‘áƒ
            if url.startswith('https://'):
                http_url = url.replace('https://', 'http://', 1)
                try:
                    async with self.session.get(http_url, headers={'Accept-Encoding': 'gzip, deflate'}) as response:
                        if response.status == 200:
                            content = await response.text(encoding='utf-8', errors='ignore')
                            logger.warning(f"Fallback to HTTP successful for {http_url}")
                            return content
                except Exception as e:
                    logger.error(f"HTTP fallback also failed: {e}")
            return None
            
        except aiohttp.ClientConnectorError as conn_error:
            logger.error(f"Connection Error for {url}: {conn_error}")
            return None
            
        except asyncio.TimeoutError:
            logger.error(f"Timeout Error for URL: {url}")
            return None
            
        except Exception as e:
            logger.error(f"Unexpected error fetching {url}: {str(e)}")
            # áƒ‘áƒáƒšáƒ áƒ›áƒªáƒ“áƒ”áƒšáƒáƒ‘áƒ - áƒ§áƒ•áƒ”áƒšáƒáƒ–áƒ” áƒ›áƒáƒ áƒ¢áƒ˜áƒ•áƒ˜ áƒ›áƒ˜áƒ“áƒ’áƒáƒ›áƒ
            try:
                simple_headers = {'Accept-Encoding': 'identity', 'User-Agent': 'TelegramBot/1.0'}
                async with self.session.get(url, headers=simple_headers) as response:
                    if response.status == 200:
                        content = await response.read()
                        return content.decode('utf-8', errors='ignore')
            except Exception as final_error:
                logger.error(f"Final fallback failed for {url}: {final_error}")
            return None
    
    def parse_products(self, html_content, base_url):
        """HTML-áƒ˜áƒ¡ áƒáƒáƒ áƒ¡áƒ˜áƒœáƒ’áƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ›áƒáƒ¡áƒáƒáƒáƒ•áƒ”áƒ‘áƒšáƒáƒ“"""
        soup = BeautifulSoup(html_content, 'html.parser')
        products = []
        
        # raiders.ge-áƒ¡áƒ—áƒ•áƒ˜áƒ¡ áƒ¡áƒáƒ”áƒªáƒ˜áƒáƒšáƒ£áƒ áƒ˜ áƒ¡áƒ”áƒšáƒ”áƒ¥áƒ¢áƒáƒ áƒ”áƒ‘áƒ˜
        if 'raiders.ge' in base_url:
            product_selectors = [
                '.product-item',
                '.product-card',
                '.item-product',
                '.product-container',
                '.card',
                '[data-product-id]',
                '.product-list-item'
            ]
        else:
            # áƒ–áƒáƒ’áƒáƒ“áƒ˜ áƒ¡áƒ”áƒšáƒ”áƒ¥áƒ¢áƒáƒ áƒ”áƒ‘áƒ˜
            product_selectors = [
                '.product',
                '.item',
                '.product-item',
                '.product-card',
                '[class*="product"]',
                '.card',
                '.item-card'
            ]
        
        for selector in product_selectors:
            items = soup.select(selector)
            if items and len(items) > 1:  # áƒ›áƒ˜áƒœáƒ˜áƒ›áƒ£áƒ› 2 áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜ áƒ£áƒœáƒ“áƒ áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒáƒ¡
                for item in items[:10]:  # áƒ›áƒáƒ¥áƒ¡áƒ˜áƒ›áƒ£áƒ› 10 áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜
                    product = self.extract_product_info(item, base_url)
                    if product:
                        products.append(product)
                if products:  # áƒ—áƒ£ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ”áƒ‘áƒ˜ áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ, áƒ¨áƒ”áƒ•áƒ¬áƒ§áƒ•áƒ˜áƒ¢áƒáƒ—
                    break
        
        # áƒ—áƒ£ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ”áƒ‘áƒ˜ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ, áƒ•áƒ”áƒªáƒáƒ“áƒáƒ— áƒ§áƒ•áƒ”áƒšáƒáƒ–áƒ” áƒ¤áƒáƒ áƒ—áƒ áƒ«áƒ”áƒ‘áƒœáƒ
        if not products:
            fallback_selectors = [
                'div[class*="item"]',
                'div[class*="card"]', 
                'div[class*="product"]',
                'article',
                'li[class*="item"]'
            ]
            
            for selector in fallback_selectors:
                items = soup.select(selector)
                if items:
                    for item in items[:15]:
                        product = self.extract_product_info(item, base_url)
                        if product:
                            products.append(product)
                    if products:
                        break
        
        return products
    
    def extract_product_info(self, item, base_url):
        """áƒªáƒáƒšáƒ™áƒ”áƒ£áƒšáƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜áƒ¡ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ˜áƒ¡ áƒáƒ›áƒáƒ¦áƒ”áƒ‘áƒ"""
        try:
            # áƒ¡áƒáƒ®áƒ”áƒšáƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ (raiders.ge-áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡ áƒ’áƒáƒ¤áƒáƒ áƒ—áƒáƒ”áƒ‘áƒ£áƒšáƒ˜)
            name_selectors = [
                'h1', 'h2', 'h3', 'h4',
                '.title', '.name', '.product-name', '.product-title',
                '.item-title', '.card-title',
                'a[title]',  # áƒšáƒ˜áƒœáƒ™áƒ˜áƒ¡ title áƒáƒ¢áƒ áƒ˜áƒ‘áƒ£áƒ¢áƒ˜
                '.product-info h3', '.product-info h2'
            ]
            name = None
            for selector in name_selectors:
                name_elem = item.select_one(selector)
                if name_elem:
                    name_text = name_elem.get_text(strip=True)
                    if len(name_text) > 5:  # áƒ›áƒ˜áƒœáƒ˜áƒ›áƒ£áƒ› 5 áƒ¡áƒ˜áƒ›áƒ‘áƒáƒšáƒ
                        name = name_text
                        break
                
                # title áƒáƒ¢áƒ áƒ˜áƒ‘áƒ£áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
                if not name and selector == 'a[title]':
                    title_attr = name_elem.get('title', '').strip()
                    if len(title_attr) > 5:
                        name = title_attr
                        break
            
            # áƒ¤áƒáƒ¡áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ (georgia-áƒ£áƒ áƒ˜ áƒ•áƒáƒšáƒ£áƒ¢áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡ áƒ’áƒáƒ¤áƒáƒ áƒ—áƒáƒ”áƒ‘áƒ£áƒšáƒ˜)
            price_selectors = [
                '.price', '.cost', 
                '[class*="price"]', '[class*="cost"]',
                '.amount', '.value',
                '.product-price', '.item-price'
            ]
            price = None
            for selector in price_selectors:
                price_elem = item.select_one(selector)
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    # áƒ¤áƒáƒ¡áƒ˜áƒ¡ áƒáƒ›áƒáƒ¦áƒ”áƒ‘áƒ áƒ áƒ”áƒ’áƒ”áƒ¥áƒ¡áƒ˜áƒ— - áƒšáƒáƒ áƒ˜, áƒ“áƒáƒšáƒáƒ áƒ˜, áƒ”áƒ•áƒ áƒ
                    price_patterns = [
                        r'(\d+(?:,\d{3})*(?:\.\d{2})?)\s*(?:â‚¾|áƒšáƒáƒ áƒ˜|GEL)',
                        r'(\d+(?:,\d{3})*(?:\.\d{2})?)\s*(?:\$|USD)',
                        r'(\d+(?:,\d{3})*(?:\.\d{2})?)\s*(?:â‚¬|EUR)',
                        r'(\d+(?:,\d{3})*(?:\.\d{2})?)'  # áƒ›áƒ®áƒáƒšáƒáƒ“ áƒ áƒ˜áƒªáƒ®áƒ•áƒ˜
                    ]
                    
                    for pattern in price_patterns:
                        price_match = re.search(pattern, price_text.replace(' ', ''))
                        if price_match:
                            price = price_match.group(1).replace(',', '')
                            # áƒšáƒáƒ áƒ˜áƒ¡ áƒ¡áƒ˜áƒ›áƒ‘áƒáƒšáƒ áƒ“áƒáƒ•áƒáƒ›áƒáƒ¢áƒáƒ— áƒ—áƒ£ áƒáƒ  áƒáƒ áƒ˜áƒ¡
                            if not any(symbol in price_text for symbol in ['â‚¾', 'áƒšáƒáƒ áƒ˜']):
                                price = f"{price}â‚¾"
                            break
                    if price:
                        break
            
            # áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ (áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜)
            image_url = None
            
            # áƒáƒ˜áƒ áƒ•áƒ”áƒšáƒ˜ - img áƒ”áƒšáƒ”áƒ›áƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ
            img_selectors = [
                'img',
                '.product-image img',
                '.image img', 
                '.photo img',
                '.thumbnail img',
                '.item-image img',
                '.card-img img'
            ]
            
            for selector in img_selectors:
                img_elem = item.select_one(selector)
                if img_elem:
                    # áƒ¡áƒ®áƒ•áƒáƒ“áƒáƒ¡áƒ®áƒ•áƒ src áƒáƒ¢áƒ áƒ˜áƒ‘áƒ£áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
                    img_src = (img_elem.get('src') or 
                              img_elem.get('data-src') or 
                              img_elem.get('data-lazy-src') or
                              img_elem.get('data-original') or
                              img_elem.get('data-srcset') or
                              img_elem.get('srcset'))
                    
                    if img_src:
                        # srcset-áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒ—áƒ®áƒ•áƒ”áƒ•áƒáƒ¨áƒ˜ áƒáƒ˜áƒ áƒ•áƒ”áƒšáƒ˜ URL-áƒ˜áƒ¡ áƒáƒ¦áƒ”áƒ‘áƒ
                        if ',' in img_src:
                            img_src = img_src.split(',')[0].split(' ')[0]
                        
                        # áƒ¡áƒ áƒ£áƒšáƒ˜ URL-áƒ˜áƒ¡ áƒ¨áƒ”áƒ¥áƒ›áƒœáƒ
                        if img_src.startswith('//'):
                            img_src = 'https:' + img_src
                        elif img_src.startswith('/'):
                            img_src = urljoin(base_url, img_src)
                        elif not img_src.startswith('http'):
                            img_src = urljoin(base_url, img_src)
                        
                        image_url = img_src
                        
                        # áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ•áƒáƒšáƒ˜áƒ“áƒáƒªáƒ˜áƒ
                        if self.is_valid_image_url(image_url):
                            break
            
            # áƒ—áƒ£ img áƒ”áƒšáƒ”áƒ›áƒ”áƒœáƒ¢áƒ˜ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ, background-image-áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ
            if not image_url:
                bg_selectors = ['.product-image', '.image', '.photo', '.thumbnail', '.item-image']
                for selector in bg_selectors:
                    bg_elem = item.select_one(selector)
                    if bg_elem:
                        style = bg_elem.get('style', '')
                        bg_match = re.search(r'background-image:\s*url\(["\']?(.*?)["\']?\)', style)
                        if bg_match:
                            bg_url = bg_match.group(1)
                            if bg_url.startswith('//'):
                                bg_url = 'https:' + bg_url
                            image_url = urljoin(base_url, bg_url)
                            if self.is_valid_image_url(image_url):
                                break
            
            # áƒšáƒ˜áƒœáƒ™áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ
            link_url = None
            link_elem = item.select_one('a')
            if link_elem:
                href = link_elem.get('href')
                if href:
                    if href.startswith('//'):
                        href = 'https:' + href
                    elif href.startswith('/'):
                        href = urljoin(base_url, href)
                    elif not href.startswith('http'):
                        href = urljoin(base_url, href)
                    link_url = href
            
            # áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜áƒ¡ áƒ“áƒáƒ‘áƒ áƒ£áƒœáƒ”áƒ‘áƒ
            if name and price and len(name) > 3:
                return {
                    'name': name[:150],  # áƒ¡áƒáƒ®áƒ”áƒšáƒ˜áƒ¡ áƒ¨áƒ”áƒ–áƒ¦áƒ£áƒ“áƒ•áƒ
                    'price': price,
                    'image_url': image_url,
                    'link_url': link_url
                }
                
        except Exception as e:
            logger.error(f"Error extracting product info: {str(e)}")
        
        return None
    
    def is_valid_image_url(self, url):
        """áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ URL-áƒ˜áƒ¡ áƒ•áƒáƒšáƒ˜áƒ“áƒáƒªáƒ˜áƒ"""
        if not url or len(url) < 10:
            return False
        
        # áƒ¤áƒáƒ˜áƒšáƒ˜áƒ¡ áƒ”áƒ¥áƒ¡áƒ¢áƒ”áƒœáƒ¨áƒ”áƒœáƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.svg']
        url_lower = url.lower()
        
        # URL-áƒ¨áƒ˜ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ”áƒ¥áƒ¡áƒ¢áƒ”áƒœáƒ¨áƒ”áƒœáƒ˜áƒ
        if any(url_lower.endswith(ext) for ext in image_extensions):
            return True
        
        # áƒáƒœ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ¥áƒ•áƒ”áƒ¯áƒ’áƒ£áƒ¤áƒ˜ URL-áƒ¨áƒ˜
        image_indicators = ['image', 'img', 'photo', 'picture', 'pic', 'thumb']
        if any(indicator in url_lower for indicator in image_indicators):
            return True
        
        # data: URL (base64 áƒ¡áƒ£áƒ áƒáƒ—áƒ”áƒ‘áƒ˜)
        if url.startswith('data:image'):
            return True
            
        return False
    
    async def send_products_with_images(self, update, products, website_name="áƒ¡áƒáƒ˜áƒ¢áƒ˜"):
        """áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ’áƒáƒ’áƒ–áƒáƒ•áƒœáƒ áƒ¡áƒ£áƒ áƒáƒ—áƒ”áƒ‘áƒ˜áƒ—"""
        if not products:
            await update.message.reply_text("ğŸš« áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ áƒáƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ.")
            return
        
        header_message = f"ğŸ›ï¸ *{website_name}*-áƒ˜áƒ¡ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ:\n\n"
        
        # áƒ—áƒ£ 3-áƒ–áƒ” áƒ›áƒ”áƒ¢áƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜áƒ, áƒ’áƒáƒ•áƒáƒ’áƒ–áƒáƒ•áƒœáƒáƒ— Media Group
        limited_products = products[:6]  # áƒ›áƒáƒ¥áƒ¡áƒ˜áƒ›áƒ£áƒ› 6 áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜
        
        for i, product in enumerate(limited_products, 1):
            try:
                caption = f"*{i}. {product['name']}*\n\n"
                caption += f"ğŸ’° *áƒ¤áƒáƒ¡áƒ˜:* `{product['price']}`\n"
                
                if product.get('link_url'):
                    caption += f"ğŸ”— [áƒ›áƒ”áƒ¢áƒ˜áƒ¡ áƒœáƒáƒ®áƒ•áƒ]({product['link_url']})\n"
                
                caption += f"\nğŸ“Š *áƒ¡áƒáƒ˜áƒ¢áƒ˜:* {website_name}"
                
                # áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ’áƒáƒ’áƒ–áƒáƒ•áƒœáƒ
                if product.get('image_url') and self.is_valid_image_url(product['image_url']):
                    try:
                        await update.message.reply_photo(
                            photo=product['image_url'],
                            caption=caption,
                            parse_mode='Markdown'
                        )
                    except Exception as img_error:
                        logger.warning(f"Failed to send image for {product['name']}: {img_error}")
                        # áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ’áƒáƒ áƒ”áƒ¨áƒ” áƒ’áƒáƒ’áƒ–áƒáƒ•áƒœáƒ
                        await update.message.reply_text(
                            f"ğŸ“¦ {caption}\n\nâŒ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜ áƒ•áƒ”áƒ  áƒ©áƒáƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ",
                            parse_mode='Markdown',
                            disable_web_page_preview=True
                        )
                else:
                    # áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ’áƒáƒ áƒ”áƒ¨áƒ”
                    await update.message.reply_text(
                        f"ğŸ“¦ {caption}",
                        parse_mode='Markdown',
                        disable_web_page_preview=True
                    )
                    
                # áƒ«áƒáƒšáƒ˜áƒáƒœ áƒ¡áƒ¬áƒ áƒáƒ¤áƒ˜ áƒ’áƒáƒ’áƒ–áƒáƒ•áƒœáƒ˜áƒ¡ áƒ—áƒáƒ•áƒ˜áƒ“áƒáƒœ áƒáƒ¡áƒáƒªáƒ˜áƒšáƒ”áƒ‘áƒšáƒáƒ“
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.error(f"Error sending product {i}: {str(e)}")
                continue
    
    async def check_ssl_certificate(self, url):
        """SSL áƒ¡áƒ”áƒ áƒ¢áƒ˜áƒ¤áƒ˜áƒ™áƒáƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ"""
        try:
            parsed_url = urlparse(url)
            if parsed_url.scheme != 'https':
                return True  # HTTP áƒ¡áƒáƒ˜áƒ¢áƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡ SSL áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ áƒáƒ  áƒ­áƒ˜áƒ áƒ“áƒ”áƒ‘áƒ
            
            import socket
            import ssl as ssl_module
            
            context = ssl_module.create_default_context()
            
            with socket.create_connection((parsed_url.hostname, 443), timeout=10) as sock:
                with context.wrap_socket(sock, server_hostname=parsed_url.hostname) as ssock:
                    cert = ssock.getpeercert()
                    if cert:
                        logger.info(f"SSL Certificate valid for {parsed_url.hostname}")
                        return True
            return False
            
        except Exception as e:
            logger.warning(f"SSL check failed for {url}: {e}")
            return False  # SSL áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ áƒ•áƒ”áƒ  áƒ›áƒáƒ®áƒ”áƒ áƒ®áƒ“áƒ, áƒ›áƒáƒ’áƒ áƒáƒ› áƒ’áƒáƒ•áƒáƒ’áƒ áƒ«áƒ”áƒšáƒáƒ—


# áƒ‘áƒáƒ¢áƒ˜áƒ¡ áƒ™áƒáƒ›áƒáƒœáƒ“áƒ”áƒ‘áƒ˜
class TelegramBot:
    def __init__(self, bot_token):
        self.bot_token = bot_token
        self.product_bot = ProductBot(bot_token)
        
    async def start_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Start áƒ™áƒáƒ›áƒáƒœáƒ“áƒ"""
        keyboard = [
            [InlineKeyboardButton("ğŸ›ï¸ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ", callback_data='search_products')],
            [InlineKeyboardButton("â„¹ï¸ áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ", callback_data='help')]
        ]
        reply_markup = InlineKeyboardMarkup(keyboard)
        
        welcome_text = (
            "ğŸ¤– *áƒ›áƒáƒ’áƒ”áƒ¡áƒáƒšáƒ›áƒ”áƒ‘áƒ˜áƒ—!*\n\n"
            "áƒ”áƒ¡ áƒ‘áƒáƒ¢áƒ˜ áƒ“áƒáƒ’áƒ”áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒáƒ— áƒ¡áƒáƒ˜áƒ¢áƒ”áƒ‘áƒ˜áƒ“áƒáƒœ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ›áƒáƒ«áƒ”áƒ‘áƒœáƒáƒ¨áƒ˜.\n\n"
            "ğŸ“ *áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ:*\n"
            "â€¢ áƒ’áƒáƒ›áƒáƒ’áƒ–áƒáƒ•áƒœáƒ”áƒ— áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ URL\n"
            "â€¢ áƒáƒœ áƒ’áƒáƒ›áƒáƒ˜áƒ§áƒ”áƒœáƒ”áƒ— `/search <URL>` áƒ™áƒáƒ›áƒáƒœáƒ“áƒ\n\n"
            "áƒ“áƒáƒ¬áƒ§áƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡ áƒáƒ˜áƒ áƒ©áƒ˜áƒ”áƒ— áƒ¦áƒ˜áƒšáƒáƒ™áƒ˜:"
        )
        
        await update.message.reply_text(welcome_text, reply_markup=reply_markup, parse_mode='Markdown')
    
    async def search_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Search áƒ™áƒáƒ›áƒáƒœáƒ“áƒ"""
        if not context.args:
            await update.message.reply_text("â— áƒ’áƒ—áƒ®áƒáƒ•áƒ— áƒ›áƒ˜áƒ£áƒ—áƒ˜áƒ—áƒáƒ— áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ URL\n\náƒ›áƒáƒ’áƒáƒšáƒ˜áƒ—áƒ˜: `/search https://example.com`", parse_mode='Markdown')
            return
        
        url = context.args[0]
        await self.process_website(update, url)
    
    async def handle_url_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """URL áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ"""
        text = update.message.text
        
        # URL-áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
        url_pattern = r'https?://[^\s]+'
        urls = re.findall(url_pattern, text)
        
        if urls:
            await self.process_website(update, urls[0])
        else:
            await update.message.reply_text("â— áƒ’áƒ—áƒ®áƒáƒ•áƒ— áƒ’áƒáƒ›áƒáƒ’áƒ–áƒáƒ•áƒœáƒáƒ— áƒ•áƒáƒšáƒ˜áƒ“áƒ£áƒ áƒ˜ URL")
    
    async def process_website(self, update, url):
        """áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ"""
        # URL-áƒ˜áƒ¡ áƒ•áƒáƒšáƒ˜áƒ“áƒáƒªáƒ˜áƒ
        try:
            parsed_url = urlparse(url)
            if not parsed_url.scheme:
                url = 'https://' + url
            elif parsed_url.scheme not in ['http', 'https']:
                await update.message.reply_text("â— áƒ’áƒ—áƒ®áƒáƒ•áƒ— áƒ’áƒáƒ›áƒáƒ˜áƒ§áƒ”áƒœáƒáƒ— HTTP áƒáƒœ HTTPS URL")
                return
        except Exception:
            await update.message.reply_text("â— áƒáƒ áƒáƒ¡áƒ¬áƒáƒ áƒ˜ URL áƒ¤áƒáƒ áƒ›áƒáƒ¢áƒ˜")
            return
        
        # áƒ«áƒ”áƒ‘áƒœáƒ˜áƒ¡ áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ
        search_message = await update.message.reply_text("ğŸ” áƒ•áƒ«áƒ”áƒ‘áƒœáƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒáƒ¡...")
        
        try:
            # HTTP áƒ¡áƒ”áƒ¡áƒ˜áƒ˜áƒ¡ áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ (áƒ—áƒ£ áƒáƒ  áƒáƒ áƒ˜áƒ¡)
            if not self.product_bot.session:
                await self.product_bot.init_session()
            
            # SSL áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ (HTTPS áƒ¡áƒáƒ˜áƒ¢áƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡)
            ssl_status = "ğŸ”" if url.startswith('https://') else "ğŸ”“"
            if url.startswith('https://'):
                await search_message.edit_text(f"ğŸ” áƒ•áƒ«áƒ”áƒ‘áƒœáƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒáƒ¡... {ssl_status} SSL áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ")
                ssl_valid = await self.product_bot.check_ssl_certificate(url)
                ssl_status = "âœ…ğŸ”" if ssl_valid else "âš ï¸ğŸ”"
            
            await search_message.edit_text(f"ğŸ” áƒ•áƒ«áƒ”áƒ‘áƒœáƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒáƒ¡... {ssl_status}")
            
            # áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒ›áƒáƒáƒáƒ•áƒ”áƒ‘áƒ
            html_content = await self.product_bot.fetch_website_content(url)
            
            if not html_content:
                await search_message.edit_text("âŒ áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ áƒ•áƒ”áƒ  áƒ›áƒáƒ®áƒ”áƒ áƒ®áƒ“áƒ")
                return
            
            await search_message.edit_text(f"ğŸ” áƒ•áƒáƒœáƒáƒšáƒ˜áƒ–áƒ”áƒ‘ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒáƒ¡... {ssl_status}")
            
            # áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒáƒáƒ áƒ¡áƒ˜áƒœáƒ’áƒ˜
            products = self.product_bot.parse_products(html_content, url)
            
            # áƒ«áƒ”áƒ‘áƒœáƒ˜áƒ¡ áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ¬áƒáƒ¨áƒšáƒ
            await search_message.delete()
            
            # áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜áƒ¡ áƒ¤áƒáƒ áƒ›áƒáƒ¢áƒ˜áƒ áƒ”áƒ‘áƒ áƒ“áƒ áƒ’áƒáƒ’áƒ–áƒáƒ•áƒœáƒ áƒ¡áƒ£áƒ áƒáƒ—áƒ”áƒ‘áƒ˜áƒ—
            website_name = f"{ssl_status} {urlparse(url).netloc}"
            await self.product_bot.send_products_with_images(update, products, website_name)
            
        except Exception as e:
            logger.error(f"Error processing website: {str(e)}")
            await search_message.edit_text("âŒ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ˜áƒ¡áƒáƒ¡ áƒ›áƒáƒ®áƒ“áƒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ")
    
    async def button_callback(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """áƒ¦áƒ˜áƒšáƒáƒ™áƒ”áƒ‘áƒ˜áƒ¡ callback"""
        query = update.callback_query
        await query.answer()
        
        if query.data == 'search_products':
            await query.edit_message_text(
                "ğŸ“ áƒ’áƒ—áƒ®áƒáƒ•áƒ— áƒ’áƒáƒ›áƒáƒ’áƒ–áƒáƒ•áƒœáƒáƒ— áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ URL áƒ áƒáƒ›áƒšáƒ˜áƒ“áƒáƒœáƒáƒª áƒ’áƒ¡áƒ£áƒ áƒ— áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ:\n\n"
                "áƒ›áƒáƒ’áƒáƒšáƒ˜áƒ—áƒ˜: `https://example.com`",
                parse_mode='Markdown'
            )
        elif query.data == 'help':
            help_text = (
                "ğŸ“– *áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒ”áƒ¥áƒªáƒ˜áƒ*\n\n"
                "ğŸ”¹ *áƒ™áƒáƒ›áƒáƒœáƒ“áƒ”áƒ‘áƒ˜:*\n"
                "â€¢ `/start` - áƒ‘áƒáƒ¢áƒ˜áƒ¡ áƒ’áƒáƒ¨áƒ•áƒ”áƒ‘áƒ\n"
                "â€¢ `/search <URL>` - áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ\n"
                "â€¢ `/help` - áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ\n\n"
                "ğŸ”¹ *áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ:*\n"
                "1. áƒ’áƒáƒ›áƒáƒ’áƒ–áƒáƒ•áƒœáƒ”áƒ— áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ URL\n"
                "2. áƒ‘áƒáƒ¢áƒ˜ áƒ’áƒáƒ“áƒáƒ•áƒ áƒ¡áƒáƒ˜áƒ¢áƒ–áƒ”\n"
                "3. áƒ›áƒáƒ˜áƒ«áƒ˜áƒ”áƒ‘áƒ¡ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒáƒ¡áƒ áƒ“áƒ áƒ¤áƒáƒ¡áƒ”áƒ‘áƒ¡\n"
                "4. áƒ’áƒáƒ›áƒáƒ’áƒ–áƒáƒ•áƒœáƒ˜áƒ¡ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒáƒ¡ áƒ©áƒáƒ¢áƒ¨áƒ˜\n\n"
                "ğŸ”¹ *áƒ›áƒ®áƒáƒ áƒ“áƒáƒ­áƒ”áƒ áƒ˜áƒšáƒ˜ áƒ¡áƒáƒ˜áƒ¢áƒ”áƒ‘áƒ˜:*\n"
                "â€¢ áƒ§áƒ•áƒ”áƒšáƒ áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒ áƒáƒ›áƒ”áƒšáƒ˜áƒª HTML áƒ¡áƒ¢áƒáƒœáƒ“áƒáƒ áƒ¢áƒ£áƒšáƒ˜ áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒ˜áƒ¡ áƒáƒ¥áƒ•áƒ¡"
            )
            
            back_keyboard = [[InlineKeyboardButton("ğŸ”™ áƒ£áƒ™áƒáƒœ", callback_data='back_to_menu')]]
            back_markup = InlineKeyboardMarkup(back_keyboard)
            
            await query.edit_message_text(help_text, reply_markup=back_markup, parse_mode='Markdown')
        
        elif query.data == 'back_to_menu':
            keyboard = [
                [InlineKeyboardButton("ğŸ›ï¸ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ", callback_data='search_products')],
                [InlineKeyboardButton("â„¹ï¸ áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ", callback_data='help')]
            ]
            reply_markup = InlineKeyboardMarkup(keyboard)
            
            welcome_text = (
                "ğŸ¤– *áƒ›áƒáƒ’áƒ”áƒ¡áƒáƒšáƒ›áƒ”áƒ‘áƒ˜áƒ—!*\n\
