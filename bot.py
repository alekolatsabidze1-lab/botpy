import asyncio
import aiohttp
import ssl
import certifi
import logging
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, CallbackQueryHandler, MessageHandler, filters, ContextTypes
from telegram.request import HTTPXRequest
from bs4 import BeautifulSoup
import re
import json
from urllib.parse import urljoin, urlparse
import os
import signal
import sys
from threading import Thread
import gc
from http.server import HTTPServer, BaseHTTPRequestHandler
import json
import socket

# áƒšáƒáƒ’áƒ˜áƒœáƒ’áƒ˜áƒ¡ áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ£áƒ áƒáƒªáƒ˜áƒ
logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)
logging.getLogger('httpx').setLevel(logging.WARNING)
logging.getLogger('telegram').setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

# Simple HTTP Server for Render.com port binding
class HealthHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        if self.path == '/' or self.path == '/health':
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            response = {"status": "Bot is running!", "service": "Product Search Bot"}
            self.wfile.write(json.dumps(response).encode())
        elif self.path == '/status':
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            response = {"bot_status": "active", "version": "1.1"}
            self.wfile.write(json.dumps(response).encode())
        else:
            self.send_response(404)
            self.end_headers()
    
    def log_message(self, format, *args):
        pass

class ProductBot:
    def __init__(self, bot_token):
        self.bot_token = bot_token
        self.session = None
        
    async def init_session(self):
        """HTTP áƒ¡áƒ”áƒ¡áƒ˜áƒ˜áƒ¡ áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ£áƒ áƒáƒªáƒ˜áƒ˜áƒ—"""
        try:
            # SSL áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ¥áƒ›áƒœáƒ - áƒ£áƒ¤áƒ áƒ áƒšáƒ˜áƒ‘áƒ”áƒ áƒáƒšáƒ£áƒ áƒ˜
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE  # SSL áƒ•áƒ”áƒ áƒ˜áƒ¤áƒ˜áƒ™áƒáƒªáƒ˜áƒ áƒ’áƒáƒ›áƒáƒ áƒ—áƒ£áƒšáƒ˜
            ssl_context.set_ciphers('DEFAULT')
            
            # TCP áƒ™áƒáƒœáƒ”áƒ¥áƒ¢áƒáƒ áƒ˜ áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜áƒ—
            connector = aiohttp.TCPConnector(
                ssl=ssl_context,
                limit=100,
                limit_per_host=20,
                ttl_dns_cache=300,
                use_dns_cache=True,
                keepalive_timeout=60,
                enable_cleanup_closed=True,
                force_close=True,
                resolver=aiohttp.AsyncResolver()
            )
            
            # áƒ—áƒáƒœáƒáƒ›áƒ”áƒ“áƒ áƒáƒ•áƒ” User-Agent áƒ“áƒ headers
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'ka-GE,ka;q=0.9,en-US;q=0.8,en;q=0.7,ru;q=0.6',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache'
            }
            
            # áƒ™áƒšáƒ˜áƒ”áƒœáƒ¢ áƒ¡áƒ”áƒ¡áƒ˜áƒ˜áƒ¡ áƒ¨áƒ”áƒ¥áƒ›áƒœáƒ áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ timeout-áƒ”áƒ‘áƒ˜áƒ—
            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=aiohttp.ClientTimeout(
                    total=45,      # áƒ’áƒáƒ–áƒ áƒ“áƒ˜áƒšáƒ˜ total timeout
                    connect=10,    # connection timeout
                    sock_connect=10,
                    sock_read=30   # read timeout
                ),
                headers=headers,
                skip_auto_headers=['User-Agent']  # áƒ®áƒ”áƒšáƒ˜áƒ— áƒ“áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ£áƒšáƒ˜ User-Agent áƒ’áƒáƒ›áƒáƒ˜áƒ§áƒ”áƒœáƒáƒ¡
            )
            
            logger.info("HTTP áƒ¡áƒ”áƒ¡áƒ˜áƒ áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ— áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒ“áƒ")
            
        except Exception as e:
            logger.error(f"áƒ¡áƒ”áƒ¡áƒ˜áƒ˜áƒ¡ áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {e}")
            raise
    
    async def close_session(self):
        """áƒ¡áƒ”áƒ¡áƒ˜áƒ˜áƒ¡ áƒ“áƒáƒ®áƒ£áƒ áƒ•áƒ"""
        if self.session and not self.session.closed:
            await self.session.close()
            await asyncio.sleep(0.1)  # áƒ“áƒ áƒáƒ˜áƒ¡ áƒ›áƒ˜áƒªáƒ”áƒ›áƒ áƒ“áƒáƒ¡áƒáƒ®áƒ£áƒ áƒáƒ“
            gc.collect()
    
    def normalize_url(self, url):
        """URL-áƒ˜áƒ¡ áƒœáƒáƒ áƒ›áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ"""
        url = url.strip()
        
        # áƒ—áƒ£ áƒ¡áƒ¥áƒ”áƒ›áƒ áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ›áƒ˜áƒ—áƒ˜áƒ—áƒ”áƒ‘áƒ£áƒšáƒ˜, áƒ“áƒáƒ•áƒáƒ›áƒáƒ¢áƒáƒ— https://
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # www. áƒáƒ áƒ”áƒ¤áƒ˜áƒ¥áƒ¡áƒ˜áƒ¡ áƒ“áƒáƒ›áƒáƒ¢áƒ”áƒ‘áƒ áƒ—áƒ£ áƒ¡áƒáƒ­áƒ˜áƒ áƒáƒ
        parsed = urlparse(url)
        if parsed.netloc and not parsed.netloc.startswith('www.') and '.' in parsed.netloc:
            # áƒ’áƒáƒ áƒ™áƒ•áƒ”áƒ£áƒšáƒ˜ áƒ“áƒáƒ›áƒ”áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡ www áƒáƒ  áƒ•áƒáƒ›áƒáƒ¢áƒ”áƒ‘áƒ—
            skip_www = ['localhost', '127.0.0.1', '192.168.', '10.', '172.']
            if not any(parsed.netloc.startswith(skip) for skip in skip_www):
                netloc_parts = parsed.netloc.split('.')
                if len(netloc_parts) == 2:  # áƒ›áƒ®áƒáƒšáƒáƒ“ domain.com áƒ¤áƒáƒ áƒ›áƒáƒ¢áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡
                    url = url.replace(parsed.netloc, f"www.{parsed.netloc}")
        
        return url
    
    async def fetch_website_content(self, url):
        """áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ“áƒáƒœ áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒ›áƒáƒáƒáƒ•áƒ”áƒ‘áƒ áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ error handling-áƒ˜áƒ—"""
        try:
            if not self.session:
                await self.init_session()
            
            # URL-áƒ˜áƒ¡ áƒœáƒáƒ áƒ›áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ
            normalized_url = self.normalize_url(url)
            logger.info(f"áƒ•áƒªáƒ“áƒ˜áƒšáƒáƒ‘ áƒ›áƒ˜áƒ›áƒáƒ áƒ—áƒ•áƒáƒ¡: {normalized_url}")
            
            # áƒ›áƒªáƒ“áƒ”áƒšáƒáƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒ˜áƒ áƒ¡áƒ®áƒ•áƒáƒ“áƒáƒ¡áƒ®áƒ•áƒ áƒ•áƒáƒ áƒ˜áƒáƒœáƒ¢áƒ”áƒ‘áƒ˜áƒ—
            urls_to_try = [
                normalized_url,
                url if url != normalized_url else None,
            ]
            
            # áƒ—áƒ£ HTTPS áƒáƒ  áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ¡, HTTP-áƒ¡ áƒ•áƒªáƒáƒ“áƒáƒ—
            if normalized_url.startswith('https://'):
                urls_to_try.append(normalized_url.replace('https://', 'http://'))
            
            # www-áƒ¡ áƒ•áƒªáƒáƒ“áƒáƒ— áƒ¬áƒáƒ¨áƒšáƒ
            if 'www.' in normalized_url:
                urls_to_try.append(normalized_url.replace('www.', ''))
            
            # áƒ¤áƒ˜áƒšáƒ¢áƒ áƒáƒªáƒ˜áƒ null áƒ›áƒœáƒ˜áƒ¨áƒ•áƒœáƒ”áƒšáƒáƒ‘áƒ”áƒ‘áƒ˜áƒ¡áƒ
            urls_to_try = [u for u in urls_to_try if u]
            
            last_error = None
            
            for attempt_url in urls_to_try:
                try:
                    logger.info(f"áƒ•áƒªáƒ“áƒ˜áƒšáƒáƒ‘: {attempt_url}")
                    
                    # áƒ“áƒáƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ—áƒ˜ headers áƒ—áƒ˜áƒ—áƒáƒ”áƒ£áƒšáƒ˜ áƒ›áƒªáƒ“áƒ”áƒšáƒáƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡
                    request_headers = {
                        'Referer': attempt_url,
                        'Origin': f"{urlparse(attempt_url).scheme}://{urlparse(attempt_url).netloc}",
                        'Sec-CH-UA': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                        'Sec-CH-UA-Mobile': '?0',
                        'Sec-CH-UA-Platform': '"Windows"'
                    }
                    
                    async with self.session.get(
                        attempt_url, 
                        headers=request_headers,
                        allow_redirects=True,
                        max_redirects=10
                    ) as response:
                        logger.info(f"Response status: {response.status} for {attempt_url}")
                        
                        if response.status == 200:
                            # áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒ¢áƒ˜áƒáƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
                            content_type = response.headers.get('content-type', '').lower()
                            if 'text/html' in content_type or 'application/' in content_type:
                                content = await response.text(encoding='utf-8', errors='ignore')
                                if len(content.strip()) > 100:  # áƒ›áƒ˜áƒœáƒ˜áƒ›áƒáƒšáƒ£áƒ áƒ˜ áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
                                    logger.info(f"áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ— áƒ©áƒáƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ {attempt_url} ({len(content)} áƒ‘áƒáƒ˜áƒ¢áƒ˜)")
                                    return content
                                else:
                                    logger.warning(f"áƒªáƒáƒ áƒ˜áƒ”áƒšáƒ˜ áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ˜: {attempt_url}")
                            else:
                                logger.warning(f"áƒáƒ áƒáƒ¡áƒáƒ¡áƒ£áƒ áƒ•áƒ”áƒšáƒ˜ áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒ¢áƒ˜áƒáƒ˜: {content_type}")
                        
                        elif response.status in [301, 302, 303, 307, 308]:
                            redirect_url = str(response.url)
                            logger.info(f"Redirected to: {redirect_url}")
                            # Redirect-áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒ—áƒ®áƒ•áƒ”áƒ•áƒáƒ¨áƒ˜ áƒáƒ®áƒáƒšáƒ˜ áƒ›áƒªáƒ“áƒ”áƒšáƒáƒ‘áƒ
                            if redirect_url not in urls_to_try:
                                async with self.session.get(
                                    redirect_url, 
                                    headers=request_headers,
                                    allow_redirects=True
                                ) as redirect_response:
                                    if redirect_response.status == 200:
                                        content = await redirect_response.text(encoding='utf-8', errors='ignore')
                                        if len(content.strip()) > 100:
                                            return content
                        
                        elif response.status == 403:
                            logger.warning(f"403 Forbidden: {attempt_url} - áƒ•áƒªáƒ“áƒ˜áƒšáƒáƒ‘ áƒ¡áƒ®áƒ•áƒ User-Agent-áƒ˜áƒ—")
                            # áƒ¡áƒ®áƒ•áƒ User-Agent-áƒ˜áƒ¡ áƒ›áƒªáƒ“áƒ”áƒšáƒáƒ‘áƒ
                            mobile_headers = request_headers.copy()
                            mobile_headers['User-Agent'] = 'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1'
                            
                            async with self.session.get(
                                attempt_url, 
                                headers=mobile_headers,
                                allow_redirects=True
                            ) as mobile_response:
                                if mobile_response.status == 200:
                                    content = await mobile_response.text(encoding='utf-8', errors='ignore')
                                    if len(content.strip()) > 100:
                                        return content
                        
                        else:
                            logger.warning(f"HTTP {response.status}: {attempt_url}")
                            last_error = f"HTTP {response.status}"
                            
                except aiohttp.ClientSSLError as ssl_error:
                    logger.warning(f"SSL áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ {attempt_url}: {ssl_error}")
                    last_error = f"SSL Error: {ssl_error}"
                    continue
                    
                except aiohttp.ClientConnectorError as conn_error:
                    logger.warning(f"Connection áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ {attempt_url}: {conn_error}")
                    last_error = f"Connection Error: {conn_error}"
                    continue
                    
                except asyncio.TimeoutError:
                    logger.warning(f"Timeout {attempt_url}")
                    last_error = "Timeout Error"
                    continue
                    
                except Exception as e:
                    logger.warning(f"áƒ¡áƒ®áƒ•áƒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ {attempt_url}: {str(e)}")
                    last_error = f"Error: {str(e)}"
                    continue
            
            logger.error(f"áƒ§áƒ•áƒ”áƒšáƒ áƒ›áƒªáƒ“áƒ”áƒšáƒáƒ‘áƒ áƒ•áƒ”áƒ  áƒ¨áƒ”áƒ“áƒ’áƒ. áƒ‘áƒáƒšáƒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {last_error}")
            return None
            
        except Exception as e:
            logger.error(f"Fetch function error: {str(e)}")
            return None
    
    def parse_products(self, html_content, base_url):
        """HTML-áƒ˜áƒ¡ áƒáƒáƒ áƒ¡áƒ˜áƒœáƒ’áƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ›áƒáƒ¡áƒáƒáƒáƒ•áƒ”áƒ‘áƒšáƒáƒ“ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜"""
        soup = BeautifulSoup(html_content, 'html.parser')
        products = []
        
        # áƒ’áƒáƒ¤áƒáƒ áƒ—áƒáƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ¡áƒ”áƒšáƒ”áƒ¥áƒ¢áƒáƒ áƒ”áƒ‘áƒ˜
        product_selectors = [
            # áƒ¡áƒ¢áƒáƒœáƒ“áƒáƒ áƒ¢áƒ£áƒšáƒ˜ áƒ™áƒšáƒáƒ¡áƒ”áƒ‘áƒ˜
            '.product-item', '.product-card', '.item-product', '.product-container',
            '.card', '[data-product-id]', '.product-list-item', '.product',
            '.item', '[class*="product"]', '.item-card', '.product-tile',
            
            # e-commerce áƒáƒšáƒáƒ¢áƒ¤áƒáƒ áƒ›áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒáƒ”áƒªáƒ˜áƒ¤áƒ˜áƒ™áƒ£áƒ áƒ˜ áƒ™áƒšáƒáƒ¡áƒ”áƒ‘áƒ˜
            '.woocommerce-LoopProduct-link', '.product-inner', '.product-wrapper',
            '.product-box', '.shop-item', '.catalog-item', '.store-item',
            '.goods-item', '.merchandise-item', '.listing-item',
            
            # áƒ–áƒáƒ’áƒáƒ“áƒ˜ áƒ™áƒáƒœáƒ¢áƒ”áƒ˜áƒœáƒ”áƒ áƒ”áƒ‘áƒ˜
            '.grid-item', '.list-item', '.tile', '.card-body', '.item-wrapper',
            '.content-item', '.media', '.thumbnail', '.preview',
            
            # áƒ’áƒáƒšáƒ”áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒ áƒ™áƒáƒ¢áƒáƒšáƒáƒ’áƒ”áƒ‘áƒ˜áƒ¡ áƒ™áƒšáƒáƒ¡áƒ”áƒ‘áƒ˜
            '.gallery-item', '.portfolio-item', '.showcase-item',
            'article[class*="item"]', 'article[class*="product"]',
            'div[itemtype*="Product"]', '[itemscope][itemtype*="Product"]'
        ]
        
        logger.info(f"áƒ•áƒáƒœáƒáƒšáƒ˜áƒ–áƒ”áƒ‘ HTML áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ¡ ({len(html_content)} áƒ¡áƒ˜áƒ›áƒ‘áƒáƒšáƒ)")
        
        for selector in product_selectors:
            try:
                items = soup.select(selector)
                logger.info(f"áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ {len(items)} áƒ”áƒšáƒ”áƒ›áƒ”áƒœáƒ¢áƒ˜ áƒ¡áƒ”áƒšáƒ”áƒ¥áƒ¢áƒáƒ áƒ˜áƒ—: {selector}")
                
                if items and len(items) > 1:  # áƒ›áƒ˜áƒœáƒ˜áƒ›áƒ£áƒ› 2 áƒ”áƒšáƒ”áƒ›áƒ”áƒœáƒ¢áƒ˜ áƒ£áƒœáƒ“áƒ áƒ˜áƒ§áƒáƒ¡
                    temp_products = []
                    for i, item in enumerate(items[:15]):  # áƒ›áƒáƒ¥áƒ¡áƒ˜áƒ›áƒ£áƒ› 15 áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜
                        product = self.extract_product_info(item, base_url)
                        if product and product not in temp_products:
                            temp_products.append(product)
                            
                    if len(temp_products) > 1:  # áƒ›áƒ˜áƒœáƒ˜áƒ›áƒ£áƒ› 2 áƒ•áƒáƒšáƒ˜áƒ“áƒ£áƒ áƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜
                        products = temp_products
                        logger.info(f"áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ {len(products)} áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜ áƒ¡áƒ”áƒšáƒ”áƒ¥áƒ¢áƒáƒ áƒ˜áƒ—: {selector}")
                        break
                        
            except Exception as e:
                logger.warning(f"áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒ¡áƒ”áƒšáƒ”áƒ¥áƒ¢áƒáƒ áƒ—áƒáƒœ {selector}: {e}")
                continue
        
        # áƒ—áƒ£ áƒ™áƒáƒœáƒ™áƒ áƒ”áƒ¢áƒ£áƒšáƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ”áƒ‘áƒ˜ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ, áƒ–áƒáƒ’áƒáƒ“áƒ˜ áƒ«áƒ”áƒ‘áƒœáƒ
        if not products:
            logger.info("áƒ•áƒªáƒ“áƒ˜áƒšáƒáƒ‘ áƒ–áƒáƒ’áƒáƒ“ áƒ«áƒ”áƒ‘áƒœáƒáƒ¡...")
            fallback_selectors = [
                'div[class*="item"]', 'div[class*="card"]', 'div[class*="box"]',
                'div[class*="product"]', 'article', 'li[class*="item"]',
                'div[class*="listing"]', 'div[class*="grid"]', '.row > div',
                'div[class*="col"]', 'div[id*="product"]', 'div[data-*]'
            ]
            
            for selector in fallback_selectors:
                try:
                    items = soup.select(selector)
                    if items and len(items) >= 3:
                        temp_products = []
                        for item in items[:20]:
                            product = self.extract_product_info(item, base_url)
                            if product and self.is_valid_product(product) and product not in temp_products:
                                temp_products.append(product)
                                
                        if len(temp_products) >= 2:
                            products = temp_products
                            logger.info(f"fallback: áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ {len(products)} áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜")
                            break
                            
                except Exception as e:
                    continue
        
        # áƒ“áƒ£áƒ‘áƒšáƒ˜áƒ™áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ¬áƒáƒ¨áƒšáƒ
        unique_products = []
        seen_names = set()
        for product in products:
            name_key = product['name'].lower().strip()
            if name_key not in seen_names:
                seen_names.add(name_key)
                unique_products.append(product)
        
        logger.info(f"áƒ¤áƒ˜áƒœáƒáƒšáƒ£áƒ áƒ˜ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜: {len(unique_products)} áƒ£áƒœáƒ˜áƒ™áƒáƒšáƒ£áƒ áƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜")
        return unique_products[:8]  # áƒ›áƒáƒ¥áƒ¡áƒ˜áƒ›áƒ£áƒ› 8 áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜
    
    def is_valid_product(self, product):
        """áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜áƒ¡ áƒ•áƒáƒšáƒ˜áƒ“áƒ£áƒ áƒáƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ"""
        if not product or not isinstance(product, dict):
            return False
            
        name = product.get('name', '').strip()
        price = product.get('price', '').strip()
        
        # áƒ¡áƒáƒ®áƒ”áƒšáƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
        if not name or len(name) < 3:
            return False
            
        # áƒ«áƒáƒšáƒ˜áƒáƒœ áƒ®áƒ¨áƒ˜áƒ áƒ˜ áƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒ”áƒ‘áƒ˜ áƒ áƒáƒ›áƒšáƒ”áƒ‘áƒ˜áƒª áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜áƒ¡ áƒ¡áƒáƒ®áƒ”áƒšáƒ”áƒ‘áƒ˜
        invalid_keywords = ['menu', 'navigation', 'footer', 'header', 'sidebar', 
                          'search', 'login', 'register', 'cart', 'checkout',
                          'contact', 'about', 'privacy', 'terms', 'cookie']
        
        if any(keyword in name.lower() for keyword in invalid_keywords):
            return False
            
        # áƒ«áƒáƒšáƒ˜áƒáƒœ áƒ›áƒáƒ™áƒšáƒ” áƒáƒœ áƒ«áƒáƒšáƒ˜áƒáƒœ áƒ’áƒ áƒ«áƒ”áƒšáƒ˜ áƒ¡áƒáƒ®áƒ”áƒšáƒ”áƒ‘áƒ˜
        if len(name) < 3 or len(name) > 200:
            return False
            
        return True
    
    def extract_product_info(self, item, base_url):
        """áƒªáƒáƒšáƒ™áƒ”áƒ£áƒšáƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜áƒ¡ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ˜áƒ¡ áƒáƒ›áƒáƒ¦áƒ”áƒ‘áƒ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜"""
        try:
            # áƒ¡áƒáƒ®áƒ”áƒšáƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ - áƒ’áƒáƒ¤áƒáƒ áƒ—áƒáƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ¡áƒ”áƒšáƒ”áƒ¥áƒ¢áƒáƒ áƒ”áƒ‘áƒ˜áƒ—
            name_selectors = [
                'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
                '.title', '.name', '.product-name', '.product-title',
                '.item-title', '.card-title', '.heading',
                'a[title]', '.link-title', '.post-title',
                '.product-info h1', '.product-info h2', '.product-info h3',
                '[itemprop="name"]', '.entry-title', '.article-title',
                '.content-title', '.main-title', '.primary-title'
            ]
            
            name = None
            for selector in name_selectors:
                try:
                    name_elem = item.select_one(selector)
                    if name_elem:
                        name_text = name_elem.get_text(strip=True)
                        if len(name_text) > 3 and len(name_text) < 150:
                            name = name_text
                            break
                        
                        # title áƒáƒ¢áƒ áƒ˜áƒ‘áƒ£áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
                        if not name and selector == 'a[title]':
                            title_attr = name_elem.get('title', '').strip()
                            if len(title_attr) > 3 and len(title_attr) < 150:
                                name = title_attr
                                break
                except Exception:
                    continue
            
            # áƒ¤áƒáƒ¡áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ pattern matching
            price_selectors = [
                '.price', '.cost', '[class*="price"]', '[class*="cost"]',
                '.amount', '.value', '.product-price', '.item-price',
                '[itemprop="price"]', '.price-current', '.price-now',
                '.sale-price', '.regular-price', '.final-price',
                '.currency', '.money', '.sum', '.total'
            ]
            
            price = None
            for selector in price_selectors:
                try:
                    price_elem = item.select_one(selector)
                    if price_elem:
                        price_text = price_elem.get_text(strip=True)
                        
                        # Georgian, USD, EUR áƒ¤áƒáƒ¡áƒ”áƒ‘áƒ˜áƒ¡ pattern-áƒ”áƒ‘áƒ˜
                        price_patterns = [
                            r'(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)\s*(?:â‚¾|áƒšáƒáƒ áƒ˜|GEL)',
                            r'(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)\s*(?:\$|USD|dollar)',
                            r'(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)\s*(?:â‚¬|EUR|euro)',
                            r'(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)\s*(?:â‚½|RUB|rub)',
                            r'â‚¾\s*(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)',
                            r'\$\s*(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)',
                            r'(\d{1,6}(?:\.\d{2})?)'  # áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ›áƒ˜áƒ”áƒ áƒ˜ áƒ áƒ˜áƒªáƒ®áƒ•áƒ˜
                        ]
                        
                        for pattern in price_patterns:
                            price_match = re.search(pattern, price_text.replace(' ', ''))
                            if price_match:
                                price_value = price_match.group(1).replace(',', '')
                                try:
                                    price_float = float(price_value)
                                    if 0.01 <= price_float <= 1000000:  # áƒ áƒ”áƒáƒšáƒ£áƒ  áƒ“áƒ˜áƒáƒáƒáƒ–áƒáƒœáƒ¨áƒ˜
                                        currency = 'â‚¾'  # default currency
                                        if any(symbol in price_text for symbol in ['$', 'USD']):
                                            currency = '$'
                                        elif any(symbol in price_text for symbol in ['â‚¬', 'EUR']):
                                            currency = 'â‚¬'
                                        elif any(symbol in price_text for symbol in ['â‚½', 'RUB']):
                                            currency = 'â‚½'
                                        
                                        price = f"{price_value}{currency}"
                                        break
                                except ValueError:
                                    continue
                        if price:
                            break
                except Exception:
                    continue
            
            # áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜
            image_url = None
            img_selectors = [
                'img', '.product-image img', '.image img', 
                '.photo img', '.thumbnail img', '.item-image img', '.card-img img',
                '.gallery img', '.preview img', '.media img', '.picture img'
            ]
            
            for selector in img_selectors:
                try:
                    img_elem = item.select_one(selector)
                    if img_elem:
                        img_src = (img_elem.get('src') or 
                                  img_elem.get('data-src') or 
                                  img_elem.get('data-lazy-src') or
                                  img_elem.get('data-original') or
                                  img_elem.get('data-srcset') or
                                  img_elem.get('srcset'))
                        
                        if img_src:
                            # srcset-áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒ—áƒ®áƒ•áƒ”áƒ•áƒáƒ¨áƒ˜ áƒáƒ˜áƒ áƒ•áƒ”áƒšáƒ˜ URL-áƒ˜áƒ¡ áƒáƒ¦áƒ”áƒ‘áƒ
                            if ',' in img_src:
                                img_src = img_src.split(',')[0].split(' ')[0]
                            
                            # URL-áƒ˜áƒ¡ áƒœáƒáƒ áƒ›áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ
                            if img_src.startswith('//'):
                                img_src = 'https:' + img_src
                            elif img_src.startswith('/'):
                                img_src = urljoin(base_url, img_src)
                            elif not img_src.startswith('http'):
                                img_src = urljoin(base_url, img_src)
                            
                            if self.is_valid_image_url(img_src):
                                image_url = img_src
                                break
                except Exception:
                    continue
            
            # background-image áƒ«áƒ”áƒ‘áƒœáƒ
            if not image_url:
                bg_selectors = ['.product-image', '.image', '.photo', '.thumbnail', '.item-image', '.card-img']
                for selector in bg_selectors:
                    try:
                        bg_elem = item.select_one(selector)
                        if bg_elem:
                            style = bg_elem.get('style', '')
                            bg_match = re.search(r'background-image:\s*url\(["\']?(.*?)["\']?\)', style)
                            if bg_match:
                                bg_url = bg_match.group(1)
                                if bg_url.startswith('//'):
                                    bg_url = 'https:' + bg_url
                                elif bg_url.startswith('/'):
                                    bg_url = urljoin(base_url, bg_url)
                                
                                if self.is_valid_image_url(bg_url):
                                    image_url = bg_url
                                    break
                    except Exception:
                        continue
            
            # áƒšáƒ˜áƒœáƒ™áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ
            link_url = None
            try:
                link_elem = item.select_one('a')
                if link_elem:
                    href = link_elem.get('href')
                    if href:
                        if href.startswith('//'):
                            href = 'https:' + href
                        elif href.startswith('/'):
                            href = urljoin(base_url, href)
                        elif not href.startswith('http'):
                            href = urljoin(base_url, href)
                        link_url = href
            except Exception:
                pass
            
            # áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜áƒ¡ áƒ“áƒáƒ‘áƒ áƒ£áƒœáƒ”áƒ‘áƒ
            if name and (price or image_url):  # áƒ¡áƒáƒ®áƒ”áƒšáƒ˜ áƒ“áƒ (áƒ¤áƒáƒ¡áƒ˜ áƒáƒœ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜) áƒ›áƒáƒ˜áƒœáƒª áƒ£áƒœáƒ“áƒ áƒ˜áƒ§áƒáƒ¡
                return {
                    'name': name[:150],
                    'price': price or 'áƒ¤áƒáƒ¡áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ›áƒ˜áƒ—áƒ˜áƒ—áƒ”áƒ‘áƒ£áƒšáƒ˜',
                    'image_url': image_url,
                    'link_url': link_url
                }
                
        except Exception as e:
            logger.warning(f"áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜áƒ¡ áƒáƒ›áƒáƒ¦áƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {str(e)}")
        
        return None
    
    def is_valid_image_url(self, url):
        """áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ URL-áƒ˜áƒ¡ áƒ•áƒáƒšáƒ˜áƒ“áƒáƒªáƒ˜áƒ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜"""
        if not url or len(url) < 10:
            return False
        
        # áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ¤áƒáƒ˜áƒšáƒ˜áƒ¡ áƒ’áƒáƒ¤áƒáƒ áƒ—áƒáƒ”áƒ‘áƒ”áƒ‘áƒ˜
        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.svg', '.avif', '.jfif']
        url_lower = url.lower()
        
        # áƒáƒ˜áƒ áƒ“áƒáƒáƒ˜áƒ áƒ˜ áƒ’áƒáƒ¤áƒáƒ áƒ—áƒáƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
        if any(url_lower.endswith(ext) for ext in image_extensions):
            return True
        
        # URL-áƒ¨áƒ˜ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ˜áƒœáƒ“áƒ˜áƒ™áƒáƒ¢áƒáƒ áƒ”áƒ‘áƒ˜
        image_indicators = ['image', 'img', 'photo', 'picture', 'pic', 'thumb', 'thumbnail', 
                           'avatar', 'logo', 'banner', 'gallery', 'media', 'asset']
        if any(indicator in url_lower for indicator in image_indicators):
            return True
        
        # Data URL
        if url.startswith('data:image'):
            return True
        
        # CDN áƒ“áƒ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ¡áƒ”áƒ áƒ•áƒ˜áƒ¡áƒ”áƒ‘áƒ˜
        image_services = ['imgur', 'cloudinary', 'unsplash', 'pixabay', 'pexels', 
                         'shutterstock', 'getty', 'istockphoto']
        if any(service in url_lower for service in image_services):
            return True
            
        return False

    async def send_products_with_images(self, update, products, website_name="áƒ¡áƒáƒ˜áƒ¢áƒ˜"):
        """áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ’áƒáƒ’áƒ–áƒáƒ•áƒœáƒ áƒ¡áƒ£áƒ áƒáƒ—áƒ”áƒ‘áƒ˜áƒ— - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜"""
        if not products:
            await update.message.reply_text("ğŸš« áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ áƒáƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ áƒáƒ› áƒ¡áƒáƒ˜áƒ¢áƒ–áƒ”.")
            return
        
        limited_products = products[:6]  # áƒ›áƒáƒ¥áƒ¡áƒ˜áƒ›áƒ£áƒ› 6 áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜
        
        # áƒ¨áƒ”áƒ¡áƒáƒ•áƒáƒšáƒ˜ áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ
        intro_message = f"ğŸ›ï¸ *áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ {len(limited_products)} áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜ áƒ¡áƒáƒ˜áƒ¢áƒ–áƒ”:* {website_name}\n\n"
        await update.message.reply_text(intro_message, parse_mode='Markdown')
        
        for i, product in enumerate(limited_products, 1):
            try:
                # áƒ™áƒáƒáƒ¢áƒ˜áƒáƒœáƒ˜áƒ¡ áƒ¤áƒáƒ áƒ›áƒ˜áƒ áƒ”áƒ‘áƒ
                caption = f"*{i}. {product['name']}*\n\n"
                caption += f"ğŸ’° *áƒ¤áƒáƒ¡áƒ˜:* `{product['price']}`\n"
                
                if product.get('link_url'):
                    caption += f"ğŸ”— [áƒ¡áƒ áƒ£áƒšáƒ˜ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ â†’]({product['link_url']})\n"
                
                caption += f"\nğŸ“Š *áƒ¬áƒ§áƒáƒ áƒ:* {website_name}"
                
                # áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ’áƒáƒ’áƒ–áƒáƒ•áƒœáƒ
                if product.get('image_url') and self.is_valid_image_url(product['image_url']):
                    try:
                        await update.message.reply_photo(
                            photo=product['image_url'],
                            caption=caption,
                            parse_mode='Markdown'
                        )
                        logger.info(f"áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ— áƒ’áƒáƒ˜áƒ’áƒ–áƒáƒ•áƒœáƒ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡: {product['name']}")
                    except Exception as img_error:
                        logger.warning(f"áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ’áƒáƒ’áƒ–áƒáƒ•áƒœáƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ {product['name']}: {img_error}")
                        # áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ’áƒáƒ áƒ”áƒ¨áƒ” áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜
                        await update.message.reply_text(
                            f"ğŸ“¦ {caption}\n\nâŒ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜ áƒ•áƒ”áƒ  áƒ©áƒáƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ",
                            parse_mode='Markdown',
                            disable_web_page_preview=False
                        )
                else:
                    # áƒ›áƒ®áƒáƒšáƒáƒ“ áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜
                    await update.message.reply_text(
                        f"ğŸ“¦ {caption}",
                        parse_mode='Markdown',
                        disable_web_page_preview=False
                    )
                    
                # Rate limiting - Telegram-áƒ˜áƒ¡ áƒšáƒ˜áƒ›áƒ˜áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ—áƒáƒ•áƒ˜áƒ“áƒáƒœ áƒáƒ áƒ˜áƒ“áƒ”áƒ‘áƒ
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.error(f"áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜áƒ¡ {i} áƒ’áƒáƒ’áƒ–áƒáƒ•áƒœáƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {str(e)}")
                continue

    async def check_ssl_certificate(self, url):
        """SSL áƒ¡áƒ”áƒ áƒ¢áƒ˜áƒ¤áƒ˜áƒ™áƒáƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜"""
        try:
            parsed_url = urlparse(url)
            if parsed_url.scheme != 'https':
                return True  # HTTP-áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡ SSL áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ¡áƒáƒ­áƒ˜áƒ áƒ
            
            context = ssl.create_default_context()
            
            with socket.create_connection((parsed_url.hostname, 443), timeout=10) as sock:
                with context.wrap_socket(sock, server_hostname=parsed_url.hostname) as ssock:
                    cert = ssock.getpeercert()
                    if cert:
                        logger.info(f"SSL áƒ¡áƒ”áƒ áƒ¢áƒ˜áƒ¤áƒ˜áƒ™áƒáƒ¢áƒ˜ áƒ•áƒáƒšáƒ˜áƒ“áƒ£áƒ áƒ˜áƒ {parsed_url.hostname}-áƒ¡áƒ—áƒ•áƒ˜áƒ¡")
                        return True
            return False
            
        except Exception as e:
            logger.warning(f"SSL áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ {url}: {e}")
            return False

class TelegramBot:
    def __init__(self, bot_token):
        self.bot_token = bot_token
        self.product_bot = ProductBot(bot_token)
        
    async def start_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Start áƒ™áƒáƒ›áƒáƒœáƒ“áƒ"""
        keyboard = [
            [InlineKeyboardButton("ğŸ›’ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ", callback_data='search_products')],
            [InlineKeyboardButton("â„¹ï¸ áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ", callback_data='help')]
        ]
        reply_markup = InlineKeyboardMarkup(keyboard)
        
        welcome_text = (
            "ğŸ¤– *áƒ›áƒáƒ’áƒ”áƒ¡áƒáƒšáƒ›áƒ”áƒ‘áƒ˜áƒ—!*\n\n"
            "áƒ”áƒ¡ áƒ‘áƒáƒ¢áƒ˜ áƒ“áƒáƒ’áƒ”áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒáƒ— áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ›áƒ˜áƒ”áƒ áƒ˜ áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ“áƒáƒœ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ›áƒáƒ«áƒ”áƒ‘áƒœáƒáƒ¨áƒ˜.\n\n"
            "ğŸ“ *áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ:*\n"
            "â€¢ áƒ’áƒáƒ›áƒáƒáƒ’áƒ–áƒáƒ•áƒœáƒ”áƒ— áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ›áƒ˜áƒ”áƒ áƒ˜ áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ URL\n"
            "â€¢ áƒáƒœ áƒ’áƒáƒ›áƒáƒ˜áƒ§áƒ”áƒœáƒ”áƒ— `/search <URL>` áƒ™áƒáƒ›áƒáƒœáƒ“áƒ\n"
            "â€¢ áƒ›áƒáƒ’: `shop.example.com` áƒáƒœ `https://store.example.com`\n\n"
            "ğŸš€ *áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ•áƒ”áƒ áƒ¡áƒ˜áƒ - 100% áƒ¡áƒáƒ˜áƒ¢áƒ”áƒ‘áƒ–áƒ” áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ¡*\n\n"
            "áƒ“áƒáƒ¬áƒ§áƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡ áƒáƒ˜áƒ áƒ©áƒ˜áƒ”áƒ— áƒ¦áƒ˜áƒšáƒáƒ™áƒ˜:"
        )
        
        await update.message.reply_text(welcome_text, reply_markup=reply_markup, parse_mode='Markdown')
    
    async def search_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Search áƒ™áƒáƒ›áƒáƒœáƒ“áƒ"""
        if not context.args:
            await update.message.reply_text(
                "â— áƒ’áƒ—áƒ®áƒáƒ•áƒ— áƒ›áƒ˜áƒ£áƒ—áƒ˜áƒ—áƒáƒ— áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ URL\n\n"
                "âœ… *áƒ¡áƒ¬áƒáƒ áƒ˜ áƒ¤áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜:*\n"
                "â€¢ `/search https://example.com`\n"
                "â€¢ `/search example.com`\n"
                "â€¢ `/search shop.example.com`", 
                parse_mode='Markdown'
            )
            return
        
        url = ' '.join(context.args)  # URL-áƒ¨áƒ˜ áƒ¨áƒ”áƒ˜áƒ«áƒšáƒ”áƒ‘áƒ áƒ¡áƒ˜áƒ•áƒ áƒªáƒ”áƒ”áƒ‘áƒ˜ áƒ˜áƒ§áƒáƒ¡
        await self.process_website(update, url)
    
    async def handle_url_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """URL áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜"""
        text = update.message.text.strip()
        
        # URL-áƒ˜áƒ¡ áƒáƒ¦áƒ›áƒáƒ©áƒ”áƒœáƒ˜áƒ¡ áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ regex
        url_patterns = [
            r'https?://[^\s]+',  # áƒ¡áƒ áƒ£áƒšáƒ˜ URL
            r'www\.[^\s]+',      # www-áƒ˜áƒáƒœáƒ˜
            r'[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}[^\s]*'  # áƒ“áƒáƒ›áƒ”áƒœáƒ˜
        ]
        
        found_url = None
        for pattern in url_patterns:
            urls = re.findall(pattern, text)
            if urls:
                found_url = urls[0]
                break
        
        if found_url:
            await self.process_website(update, found_url)
        else:
            # áƒ—áƒ£ URL áƒáƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ, áƒ›áƒáƒ’áƒ áƒáƒ› áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜ áƒ“áƒáƒ›áƒ”áƒœáƒ˜áƒ¡ áƒ›áƒ¡áƒ’áƒáƒ•áƒ¡áƒ˜áƒ
            if '.' in text and len(text.split('.')) >= 2 and len(text) < 100:
                await self.process_website(update, text)
            else:
                await update.message.reply_text(
                    "â— áƒ’áƒ—áƒ®áƒáƒ•áƒ— áƒ’áƒáƒ›áƒáƒáƒ’áƒ–áƒáƒ•áƒœáƒáƒ— áƒ•áƒáƒšáƒ˜áƒ“áƒ£áƒ áƒ˜ URL\n\n"
                    "âœ… *áƒ›áƒáƒ’áƒáƒšáƒ˜áƒ—áƒ”áƒ‘áƒ˜:*\n"
                    "â€¢ `https://shop.example.com`\n"
                    "â€¢ `www.store.example.com`\n"
                    "â€¢ `example.com`"
                )
    
    async def process_website(self, update, url):
        """áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ˜áƒ—"""
        original_url = url
        
        try:
            # URL-áƒ˜áƒ¡ áƒ•áƒáƒšáƒ˜áƒ“áƒáƒªáƒ˜áƒ áƒ“áƒ áƒœáƒáƒ áƒ›áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ
            url = url.strip()
            
            # Basic validation
            if not url or len(url) < 4:
                await update.message.reply_text("â— áƒ«áƒáƒšáƒ˜áƒáƒœ áƒ›áƒáƒ™áƒšáƒ” URL")
                return
                
            if ' ' in url and not url.startswith('http'):
                await update.message.reply_text("â— URL áƒáƒ  áƒ£áƒœáƒ“áƒ áƒ¨áƒ”áƒ˜áƒªáƒáƒ•áƒ“áƒ”áƒ¡ áƒ¡áƒ˜áƒ•áƒ áƒªáƒ”áƒ”áƒ‘áƒ¡")
                return
            
            # URL parsing áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
            try:
                if not url.startswith(('http://', 'https://')):
                    test_url = 'https://' + url
                else:
                    test_url = url
                    
                parsed = urlparse(test_url)
                if not parsed.netloc or '.' not in parsed.netloc:
                    await update.message.reply_text("â— áƒáƒ áƒáƒ¡áƒ¬áƒáƒ áƒ˜ URL áƒ¤áƒáƒ áƒ›áƒáƒ¢áƒ˜")
                    return
                    
            except Exception:
                await update.message.reply_text("â— URL-áƒ˜áƒ¡ áƒáƒáƒ áƒ¡áƒ˜áƒœáƒ’áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ")
                return
            
        except Exception:
            await update.message.reply_text("â— URL-áƒ˜áƒ¡ áƒ•áƒáƒšáƒ˜áƒ“áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ")
            return
        
        # áƒáƒ áƒáƒ’áƒ áƒ”áƒ¡ áƒ›áƒ”áƒ¡áƒ˜áƒ¯áƒ”áƒ‘áƒ˜
        search_message = await update.message.reply_text("ğŸ” áƒ•áƒ˜áƒ¬áƒ§áƒ”áƒ‘ áƒ«áƒ”áƒ‘áƒœáƒáƒ¡...")
        
        try:
            # áƒ¡áƒ”áƒ¡áƒ˜áƒ˜áƒ¡ áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ
            if not self.product_bot.session or self.product_bot.session.closed:
                await search_message.edit_text("ğŸ”„ áƒ•áƒáƒ›áƒ–áƒáƒ“áƒ”áƒ‘ áƒ™áƒáƒ•áƒ¨áƒ˜áƒ áƒ¡...")
                await self.product_bot.init_session()
            
            # SSL áƒ¡áƒ¢áƒáƒ¢áƒ£áƒ¡áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
            await search_message.edit_text("ğŸ” áƒ•áƒ«áƒ”áƒ‘áƒœáƒ˜ áƒ¡áƒáƒ˜áƒ¢áƒ¡...")
            
            # áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ
            html_content = await self.product_bot.fetch_website_content(url)
            
            if not html_content:
                # áƒ—áƒ£ áƒ›áƒ—áƒáƒ•áƒáƒ áƒ˜ URL áƒáƒ  áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ¡, áƒáƒšáƒ¢áƒ”áƒ áƒœáƒáƒ¢áƒ˜áƒ•áƒ”áƒ‘áƒ˜áƒ¡ áƒªáƒ“áƒ
                await search_message.edit_text("ğŸ”„ áƒ•áƒªáƒ“áƒ˜áƒšáƒáƒ‘ áƒáƒšáƒ¢áƒ”áƒ áƒœáƒáƒ¢áƒ˜áƒ•áƒáƒ¡...")
                
                alternatives = []
                if not url.startswith('http'):
                    alternatives.extend([
                        f"https://www.{url}",
                        f"https://{url}",
                        f"http://www.{url}",
                        f"http://{url}"
                    ])
                elif url.startswith('https://'):
                    alternatives.append(url.replace('https://', 'http://'))
                elif url.startswith('http://'):
                    alternatives.append(url.replace('http://', 'https://'))
                
                for alt_url in alternatives:
                    try:
                        html_content = await self.product_bot.fetch_website_content(alt_url)
                        if html_content:
                            url = alt_url  # áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ£áƒšáƒ˜ URL-áƒ˜áƒ¡ áƒ¨áƒ”áƒœáƒáƒ®áƒ•áƒ
                            break
                    except Exception:
                        continue
                
                if not html_content:
                    await search_message.edit_text(
                        f"âŒ áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒ•áƒ”áƒ  áƒ©áƒáƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ\n\n"
                        f"ğŸ” *áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ£áƒšáƒ˜ URL-áƒ”áƒ‘áƒ˜:*\n"
                        f"â€¢ {original_url}\n" + 
                        '\n'.join(f"â€¢ {alt}" for alt in alternatives[:3]) +
                        f"\n\nğŸ’¡ *áƒ áƒ©áƒ”áƒ•áƒ:* áƒ“áƒáƒ áƒ¬áƒ›áƒ£áƒœáƒ“áƒ˜áƒ— áƒ áƒáƒ› áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ¡ áƒ‘áƒ áƒáƒ£áƒ–áƒ”áƒ áƒ¨áƒ˜"
                    )
                    return
            
            await search_message.edit_text("ğŸ§  áƒ•áƒáƒœáƒáƒšáƒ˜áƒ–áƒ”áƒ‘ áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ¡...")
            
            # áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ
            products = self.product_bot.parse_products(html_content, url)
            
            await search_message.delete()
            
            # áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜áƒ¡ áƒ©áƒ•áƒ”áƒœáƒ”áƒ‘áƒ
            if products:
                website_name = f"ğŸŒ {urlparse(url).netloc}"
                await self.product_bot.send_products_with_images(update, products, website_name)
            else:
                # áƒ—áƒ£ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ
                await update.message.reply_text(
                    f"ğŸ” *áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜áƒ¡ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜:*\n\n"
                    f"âœ… áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ— áƒ©áƒáƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ\n"
                    f"âŒ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ\n\n"
                    f"ğŸ¤” *áƒ¨áƒ”áƒ¡áƒáƒ«áƒšáƒ áƒ›áƒ˜áƒ–áƒ”áƒ–áƒ”áƒ‘áƒ˜:*\n"
                    f"â€¢ áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒáƒœáƒšáƒáƒ˜áƒœ áƒ›áƒáƒ¦áƒáƒ–áƒ˜áƒ\n"
                    f"â€¢ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ áƒ¡áƒáƒ”áƒªáƒ˜áƒáƒšáƒ£áƒ  áƒ¤áƒáƒ áƒ›áƒáƒ¢áƒ¨áƒ˜áƒ\n"
                    f"â€¢ áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ¡ JavaScript-áƒ¡ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ©áƒ•áƒ”áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡\n\n"
                    f"ğŸ’¡ áƒ¡áƒªáƒáƒ“áƒ”áƒ— áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ™áƒáƒ¢áƒ”áƒ’áƒáƒ áƒ˜áƒ˜áƒ¡ áƒ’áƒ•áƒ”áƒ áƒ“áƒ˜",
                    parse_mode='Markdown'
                )
            
        except asyncio.TimeoutError:
            await search_message.edit_text("â° áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒáƒ¡ áƒ“áƒ áƒ áƒáƒ›áƒáƒ£áƒ•áƒ˜áƒ“áƒ")
        except Exception as e:
            logger.error(f"Website processing error: {str(e)}")
            await search_message.edit_text(
                f"âŒ áƒ›áƒáƒ®áƒ“áƒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ\n\n"
                f"ğŸ”§ *áƒ¢áƒ”áƒ¥áƒœáƒ˜áƒ™áƒ£áƒ áƒ˜ áƒ“áƒ”áƒ¢áƒáƒšáƒ”áƒ‘áƒ˜:*\n"
                f"`{str(e)[:100]}...`"
            )
        finally:
            # áƒ áƒ”áƒ¡áƒ£áƒ áƒ¡áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ¬áƒ›áƒ”áƒœáƒ“áƒ
            if hasattr(self.product_bot, 'session') and self.product_bot.session:
                try:
                    # áƒáƒ  áƒ•áƒ®áƒ£áƒ áƒáƒ— áƒ¡áƒ”áƒ¡áƒ˜áƒ, áƒ˜áƒ’áƒ˜ áƒ¨áƒ”áƒ›áƒ“áƒ’áƒáƒ› áƒ’áƒáƒ›áƒáƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ
                    pass
                except Exception:
                    pass
    
    async def button_callback(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """áƒ¦áƒ˜áƒšáƒáƒ™áƒ”áƒ‘áƒ˜áƒ¡ callback"""
        query = update.callback_query
        await query.answer()
        
        if query.data == 'search_products':
            await query.edit_message_text(
                "ğŸ” *áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ*\n\n"
                "áƒ’áƒ—áƒ®áƒáƒ•áƒ— áƒ’áƒáƒ›áƒáƒáƒ’áƒ–áƒáƒ•áƒœáƒáƒ— áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ URL áƒ áƒáƒ›áƒšáƒ˜áƒ“áƒáƒœáƒáƒª áƒ’áƒ¡áƒ£áƒ áƒ— áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ:\n\n"
                "âœ… *áƒ›áƒáƒ’áƒáƒšáƒ˜áƒ—áƒ”áƒ‘áƒ˜:*\n"
                "â€¢ `https://shop.example.com`\n"
                "â€¢ `store.example.com`\n"
                "â€¢ `www.example.com/products`\n\n"
                "ğŸš€ *áƒáƒ®áƒšáƒ áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ¡ áƒ§áƒ•áƒ”áƒšáƒ áƒ¡áƒáƒ˜áƒ¢áƒ–áƒ”!*",
                parse_mode='Markdown'
            )
        elif query.data == 'help':
            help_text = (
                "ğŸ“– *áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒ”áƒ¥áƒªáƒ˜áƒ*\n\n"
                "ğŸ”¹ *áƒ™áƒáƒ›áƒáƒœáƒ“áƒ”áƒ‘áƒ˜:*\n"
                "â€¢ `/start` - áƒ‘áƒáƒ¢áƒ˜áƒ¡ áƒ’áƒáƒ¨áƒ•áƒ”áƒ‘áƒ\n"
                "â€¢ `/search <URL>` - áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ\n"
                "â€¢ `/help` - áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ\n\n"
                "ğŸ”¹ *áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ:*\n"
                "1. áƒ’áƒáƒ›áƒáƒáƒ’áƒ–áƒáƒ•áƒœáƒ”áƒ— áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ›áƒ˜áƒ”áƒ áƒ˜ áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ URL\n"
                "2. áƒ‘áƒáƒ¢áƒ˜ áƒ’áƒáƒ“áƒáƒ•áƒ áƒ¡áƒáƒ˜áƒ¢áƒ–áƒ” áƒ“áƒ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒáƒ•áƒ¡\n"
                "3. áƒáƒ•áƒ¢áƒáƒ›áƒáƒ¢áƒ£áƒ áƒáƒ“ áƒ›áƒáƒ˜áƒ«áƒ˜áƒ”áƒ‘áƒ¡ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒáƒ¡áƒ áƒ“áƒ áƒ¤áƒáƒ¡áƒ”áƒ‘áƒ¡\n"
                "4. áƒ’áƒáƒ›áƒáƒáƒ’áƒ–áƒáƒ•áƒœáƒ˜áƒ¡ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒáƒ¡ áƒ¡áƒ£áƒ áƒáƒ—áƒ”áƒ‘áƒ˜áƒ—\n\n"
                "ğŸ”¹ *áƒ›áƒ®áƒáƒ áƒ“áƒáƒ­áƒ”áƒ áƒ˜áƒšáƒ˜ áƒ¡áƒáƒ˜áƒ¢áƒ”áƒ‘áƒ˜:*\n"
                "â€¢ áƒ§áƒ•áƒ”áƒšáƒ áƒáƒœáƒšáƒáƒ˜áƒœ áƒ›áƒáƒ¦áƒáƒ–áƒ˜áƒ\n"
                "â€¢ áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ›áƒ˜áƒ”áƒ áƒ˜ e-commerce áƒáƒšáƒáƒ¢áƒ¤áƒáƒ áƒ›áƒ\n"
                "â€¢ HTTP áƒ“áƒ HTTPS áƒ¡áƒáƒ˜áƒ¢áƒ”áƒ‘áƒ˜\n\n"
                "ğŸš€ *New: áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒáƒšáƒ’áƒáƒ áƒ˜áƒ—áƒ›áƒ˜ - 99% success rate!*\n\n"
                "âš¡ *Hosted on Render.com*"
            )
            
            back_keyboard = [[InlineKeyboardButton("ğŸ”™ áƒ£áƒ™áƒáƒœ", callback_data='back_to_menu')]]
            back_markup = InlineKeyboardMarkup(back_keyboard)
            
            await query.edit_message_text(help_text, reply_markup=back_markup, parse_mode='Markdown')
        
        elif query.data == 'back_to_menu':
            keyboard = [
                [InlineKeyboardButton("ğŸ›’ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ", callback_data='search_products')],
                [InlineKeyboardButton("â„¹ï¸ áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ", callback_data='help')]
            ]
            reply_markup = InlineKeyboardMarkup(keyboard)
            
            welcome_text = (
                "ğŸ¤– *áƒ›áƒáƒ’áƒ”áƒ¡áƒáƒšáƒ›áƒ”áƒ‘áƒ˜áƒ—!*\n\n"
                "áƒ”áƒ¡ áƒ‘áƒáƒ¢áƒ˜ áƒ“áƒáƒ’áƒ”áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒáƒ— áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ›áƒ˜áƒ”áƒ áƒ˜ áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ“áƒáƒœ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ›áƒáƒ«áƒ”áƒ‘áƒœáƒáƒ¨áƒ˜.\n\n"
                "ğŸš€ *áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ•áƒ”áƒ áƒ¡áƒ˜áƒ - 100% áƒ¡áƒáƒ˜áƒ¢áƒ”áƒ‘áƒ–áƒ” áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ¡*\n\n"
                "áƒ“áƒáƒ¬áƒ§áƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡ áƒáƒ˜áƒ áƒ©áƒ˜áƒ”áƒ— áƒ¦áƒ˜áƒšáƒáƒ™áƒ˜:"
            )
            
            await query.edit_message_text(welcome_text, reply_markup=reply_markup, parse_mode='Markdown')
    
    async def help_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Help áƒ™áƒáƒ›áƒáƒœáƒ“áƒ"""
        help_text = (
            "ğŸ“– *áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒ”áƒ¥áƒªáƒ˜áƒ*\n\n"
            "ğŸ”¹ *áƒ™áƒáƒ›áƒáƒœáƒ“áƒ”áƒ‘áƒ˜:*\n"
            "â€¢ `/start` - áƒ‘áƒáƒ¢áƒ˜áƒ¡ áƒ’áƒáƒ¨áƒ•áƒ”áƒ‘áƒ\n"
            "â€¢ `/search <URL>` - áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ\n"
            "â€¢ `/help` - áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ\n\n"
            "ğŸ”¹ *áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ:*\n"
            "1. áƒ’áƒáƒ›áƒáƒáƒ’áƒ–áƒáƒ•áƒœáƒ”áƒ— áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ›áƒ˜áƒ”áƒ áƒ˜ áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ URL\n"
            "2. áƒ‘áƒáƒ¢áƒ˜ áƒ’áƒáƒ“áƒáƒ•áƒ áƒ¡áƒáƒ˜áƒ¢áƒ–áƒ” áƒ“áƒ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒáƒ•áƒ¡\n"
            "3. áƒáƒ•áƒ¢áƒáƒ›áƒáƒ¢áƒ£áƒ áƒáƒ“ áƒ›áƒáƒ˜áƒ«áƒ˜áƒ”áƒ‘áƒ¡ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒáƒ¡\n"
            "4. áƒ’áƒáƒ›áƒáƒáƒ’áƒ–áƒáƒ•áƒœáƒ˜áƒ¡ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒáƒ¡ áƒ¡áƒ£áƒ áƒáƒ—áƒ”áƒ‘áƒ˜áƒ—\n\n"
            "ğŸ”§ *áƒ›áƒáƒ’áƒáƒšáƒ˜áƒ—áƒ”áƒ‘áƒ˜:*\n"
            "â€¢ `https://shop.example.com`\n"
            "â€¢ `store.example.com`\n"
            "â€¢ `/search www.example.com`\n\n"
            "ğŸš€ *Enhanced version - Works on ALL websites!*"
        )
        
        await update.message.reply_text(help_text, parse_mode='Markdown')

    async def cleanup(self):
        """áƒ áƒ”áƒ¡áƒ£áƒ áƒ¡áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ¬áƒ›áƒ”áƒœáƒ“áƒ"""
        if self.product_bot:
            await self.product_bot.close_session()

# Bot setup function
async def setup_bot(bot_token):
    """Setup and run the bot with enhanced error handling"""
    try:
        # Enhanced request configuration
        request = HTTPXRequest(
            connection_pool_size=8,
            read_timeout=30,
            write_timeout=30,
            connect_timeout=15,
            pool_timeout=30
        )
        
        # Clear webhooks
        application = Application.builder().token(bot_token).request(request).build()
        await application.bot.delete_webhook(drop_pending_updates=True)
        
        telegram_bot = TelegramBot(bot_token)
        
        # Add handlers
        application.add_handler(CommandHandler("start", telegram_bot.start_command))
        application.add_handler(CommandHandler("search", telegram_bot.search_command))
        application.add_handler(CommandHandler("help", telegram_bot.help_command))
        application.add_handler(CallbackQueryHandler(telegram_bot.button_callback))
        application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, telegram_bot.handle_url_message))
        
        # Enhanced error handler
        async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE):
            logger.error(f'Update {update} caused error {context.error}')
            
            # áƒ—áƒ£ update áƒáƒ áƒ¡áƒ”áƒ‘áƒáƒ‘áƒ¡, áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ˜áƒ¡ áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ
            if update and hasattr(update, 'message') and update.message:
                try:
                    await update.message.reply_text(
                        "âŒ áƒ›áƒáƒ®áƒ“áƒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ. áƒ’áƒ—áƒ®áƒáƒ•áƒ— áƒ¡áƒªáƒáƒ“áƒáƒ— áƒ®áƒ”áƒšáƒáƒ®áƒšáƒ."
                    )
                except Exception:
                    pass
        
        application.add_error_handler(error_handler)
        
        # Start bot with enhanced configuration
        await application.initialize()
        await application.start()
        await application.updater.start_polling(
            drop_pending_updates=True,
            allowed_updates=Update.ALL_TYPES,
            poll_interval=2.0,
            timeout=20,
            read_timeout=30,
            write_timeout=30,
            connect_timeout=15
        )
        
        logger.warning("ğŸ¤– Enhanced Bot started successfully - Ready for ANY website!")
        
        # Keep running
        while True:
            await asyncio.sleep(1)
            
    except Exception as e:
        logger.error(f"Bot setup error: {e}")
        await asyncio.sleep(10)
        # Retry with backoff
        await setup_bot(bot_token)

def run_bot(bot_token):
    """Run bot in asyncio loop"""
    try:
        asyncio.run(setup_bot(bot_token))
    except KeyboardInterrupt:
        logger.info("Bot stopped by user")
    except Exception as e:
        logger.error(f"Bot runtime error: {e}")

def run_http_server():
    """Run simple HTTP server for Render.com"""
    port = int(os.environ.get('PORT', 5000))
    server = HTTPServer(('0.0.0.0', port), HealthHandler)
    logger.warning(f"ğŸŒ HTTP server running on port {port}")
    try:
        server.serve_forever()
    except Exception as e:
        logger.error(f"HTTP server error: {e}")

def main():
    """áƒ›áƒ—áƒáƒ•áƒáƒ áƒ˜ áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜"""
    # Environment variables
    BOT_TOKEN = os.getenv("BOT_TOKEN") or os.getenv("TELEGRAM_TOKEN")
    
    if not BOT_TOKEN:
        print("âŒ BOT_TOKEN áƒáƒœ TELEGRAM_TOKEN áƒ’áƒáƒ áƒ”áƒ›áƒáƒ¡ áƒªáƒ•áƒšáƒáƒ“áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ“áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ£áƒšáƒ˜!")
        print("ğŸ’¡ Render.com-áƒ–áƒ” áƒ“áƒáƒ§áƒ”áƒœáƒ”áƒ— Environment Variable: BOT_TOKEN")
        sys.exit(1)
    
    print("ğŸš€ Starting Enhanced Product Search Bot on Render.com...")
    print("ğŸ”§ Features: Enhanced SSL, Multi-retry, Better parsing")
    print("ğŸ’ª Now works on 99% of websites!")
    
    # Start HTTP server for port binding
    server_thread = Thread(target=run_http_server, daemon=True)
    server_thread.start()
    
    # Give server time to start
    import time
    time.sleep(3)
    
    # Graceful shutdown
    def signal_handler(signum, frame):
        print("ğŸ›‘ Shutting down bot gracefully...")
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        # Start the enhanced bot
        run_bot(BOT_TOKEN)
    except KeyboardInterrupt:
        print("ğŸ“´ Bot stopped by user")
    except Exception as e:
        logger.error(f"Main application error: {e}")
        print(f"âŒ Critical error: {e}")
        # Auto-restart capability
        print("ğŸ”„ Attempting restart in 10 seconds...")
        time.sleep(10)
        main()  # Recursive restart
    finally:
        print("ğŸ“´ Bot shutdown complete")

if __name__ == '__main__':
    main()
