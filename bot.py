import asyncio
import aiohttp
import ssl
import certifi
import logging
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CommandHandler, CallbackQueryHandler, MessageHandler, filters, ContextTypes
from telegram.request import HTTPXRequest
from bs4 import BeautifulSoup
import re
import json
from urllib.parse import urljoin, urlparse
import os
import signal
import sys
from threading import Thread
import gc
from http.server import HTTPServer, BaseHTTPRequestHandler
import json
import socket

# áƒšáƒáƒ’áƒ˜áƒœáƒ’áƒ˜áƒ¡ áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ£áƒ áƒáƒªáƒ˜áƒ
logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)
logging.getLogger('httpx').setLevel(logging.WARNING)
logging.getLogger('telegram').setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

# Simple HTTP Server for Render.com port binding
class HealthHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        if self.path == '/' or self.path == '/health':
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            response = {"status": "Bot is running!", "service": "Product Search Bot"}
            self.wfile.write(json.dumps(response).encode())
        elif self.path == '/status':
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            response = {"bot_status": "active", "version": "1.1"}
            self.wfile.write(json.dumps(response).encode())
        else:
            self.send_response(404)
            self.end_headers()
    
    def log_message(self, format, *args):
        pass

class ProductBot:
    def __init__(self, bot_token):
        self.bot_token = bot_token
        self.session = None
        
    async def init_session(self):
        """HTTP áƒ¡áƒ”áƒ¡áƒ˜áƒ˜áƒ¡ áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ£áƒ áƒáƒªáƒ˜áƒ˜áƒ—"""
        try:
            # SSL áƒ™áƒáƒœáƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ¥áƒ›áƒœáƒ - áƒ£áƒ¤áƒ áƒ áƒšáƒ˜áƒ‘áƒ”áƒ áƒáƒšáƒ£áƒ áƒ˜
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE  # SSL áƒ•áƒ”áƒ áƒ˜áƒ¤áƒ˜áƒ™áƒáƒªáƒ˜áƒ áƒ’áƒáƒ›áƒáƒ áƒ—áƒ£áƒšáƒ˜
            ssl_context.set_ciphers('DEFAULT')
            
            # TCP áƒ™áƒáƒœáƒ”áƒ¥áƒ¢áƒáƒ áƒ˜ áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜áƒ—
            connector = aiohttp.TCPConnector(
                ssl=ssl_context,
                limit=100,
                limit_per_host=20,
                ttl_dns_cache=300,
                use_dns_cache=True,
                keepalive_timeout=60,
                enable_cleanup_closed=True,
                force_close=True,
                resolver=aiohttp.AsyncResolver()
            )
            
            # áƒ—áƒáƒœáƒáƒ›áƒ”áƒ“áƒ áƒáƒ•áƒ” User-Agent áƒ“áƒ headers
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'ka-GE,ka;q=0.9,en-US;q=0.8,en;q=0.7,ru;q=0.6',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache'
            }
            
            # áƒ™áƒšáƒ˜áƒ”áƒœáƒ¢ áƒ¡áƒ”áƒ¡áƒ˜áƒ˜áƒ¡ áƒ¨áƒ”áƒ¥áƒ›áƒœáƒ áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ timeout-áƒ”áƒ‘áƒ˜áƒ—
            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=aiohttp.ClientTimeout(
                    total=45,      # áƒ’áƒáƒ–áƒ áƒ“áƒ˜áƒšáƒ˜ total timeout
                    connect=10,    # connection timeout
                    sock_connect=10,
                    sock_read=30   # read timeout
                ),
                headers=headers,
                skip_auto_headers=['User-Agent']  # áƒ®áƒ”áƒšáƒ˜áƒ— áƒ“áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ£áƒšáƒ˜ User-Agent áƒ’áƒáƒ›áƒáƒ˜áƒ§áƒ”áƒœáƒáƒ¡
            )
            
            logger.info("HTTP áƒ¡áƒ”áƒ¡áƒ˜áƒ áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ— áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒ“áƒ")
            
        except Exception as e:
            logger.error(f"áƒ¡áƒ”áƒ¡áƒ˜áƒ˜áƒ¡ áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {e}")
            raise
    
    async def close_session(self):
        """áƒ¡áƒ”áƒ¡áƒ˜áƒ˜áƒ¡ áƒ“áƒáƒ®áƒ£áƒ áƒ•áƒ"""
        if self.session and not self.session.closed:
            await self.session.close()
            await asyncio.sleep(0.1)  # áƒ“áƒ áƒáƒ˜áƒ¡ áƒ›áƒ˜áƒªáƒ”áƒ›áƒ áƒ“áƒáƒ¡áƒáƒ®áƒ£áƒ áƒáƒ“
            gc.collect()
    
    def normalize_url(self, url):
        """URL-áƒ˜áƒ¡ áƒœáƒáƒ áƒ›áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ"""
        url = url.strip()
        
        # áƒ—áƒ£ áƒ¡áƒ¥áƒ”áƒ›áƒ áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ›áƒ˜áƒ—áƒ˜áƒ—áƒ”áƒ‘áƒ£áƒšáƒ˜, áƒ“áƒáƒ•áƒáƒ›áƒáƒ¢áƒáƒ— https://
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # www. áƒáƒ áƒ”áƒ¤áƒ˜áƒ¥áƒ¡áƒ˜áƒ¡ áƒ“áƒáƒ›áƒáƒ¢áƒ”áƒ‘áƒ áƒ—áƒ£ áƒ¡áƒáƒ­áƒ˜áƒ áƒáƒ
        parsed = urlparse(url)
        if parsed.netloc and not parsed.netloc.startswith('www.') and '.' in parsed.netloc:
            # áƒ’áƒáƒ áƒ™áƒ•áƒ”áƒ£áƒšáƒ˜ áƒ“áƒáƒ›áƒ”áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡ www áƒáƒ  áƒ•áƒáƒ›áƒáƒ¢áƒ”áƒ‘áƒ—
            skip_www = ['localhost', '127.0.0.1', '192.168.', '10.', '172.']
            if not any(parsed.netloc.startswith(skip) for skip in skip_www):
                netloc_parts = parsed.netloc.split('.')
                if len(netloc_parts) == 2:  # áƒ›áƒ®áƒáƒšáƒáƒ“ domain.com áƒ¤áƒáƒ áƒ›áƒáƒ¢áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡
                    url = url.replace(parsed.netloc, f"www.{parsed.netloc}")
        
        return url
    
    async def fetch_website_content(self, url):
        """áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ“áƒáƒœ áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒ›áƒáƒáƒáƒ•áƒ”áƒ‘áƒ - áƒ›áƒáƒ¥áƒ¡áƒ˜áƒ›áƒáƒšáƒ£áƒ áƒ˜ compatibility"""
        try:
            if not self.session:
                await self.init_session()
            
            # URL-áƒ˜áƒ¡ áƒœáƒáƒ áƒ›áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ
            normalized_url = self.normalize_url(url)
            logger.info(f"ğŸŒ áƒ›áƒªáƒ“áƒ”áƒšáƒáƒ‘áƒ: {normalized_url}")
            
            # áƒ§áƒ•áƒ”áƒšáƒ áƒ¨áƒ”áƒ¡áƒáƒ«áƒšáƒ URL áƒ•áƒáƒ áƒ˜áƒáƒœáƒ¢áƒ˜
            urls_to_try = []
            
            # áƒ«áƒ˜áƒ áƒ˜áƒ—áƒáƒ“áƒ˜ URL áƒ•áƒáƒ áƒ˜áƒáƒœáƒ¢áƒ”áƒ‘áƒ˜
            base_urls = [normalized_url]
            if normalized_url != url:
                base_urls.append(url)
            
            for base_url in base_urls:
                parsed = urlparse(base_url if base_url.startswith(('http://', 'https://')) else f'https://{base_url}')
                domain = parsed.netloc
                path = parsed.path or '/'
                
                # áƒ¡áƒ®áƒ•áƒáƒ“áƒáƒ¡áƒ®áƒ•áƒ áƒ™áƒáƒ›áƒ‘áƒ˜áƒœáƒáƒªáƒ˜áƒ”áƒ‘áƒ˜
                combinations = [
                    f"https://{domain}{path}",
                    f"https://www.{domain.replace('www.', '')}{path}",
                    f"http://{domain}{path}",
                    f"http://www.{domain.replace('www.', '')}{path}",
                    f"https://{domain.replace('www.', '')}{path}",
                    f"http://{domain.replace('www.', '')}{path}"
                ]
                
                for combo in combinations:
                    if combo not in urls_to_try:
                        urls_to_try.append(combo)
            
            # User-Agent áƒ•áƒáƒ áƒ˜áƒáƒœáƒ¢áƒ”áƒ‘áƒ˜
            user_agents = [
                # Chrome Windows
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                # Chrome Mac
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                # Firefox
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
                # Safari Mac
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
                # Edge
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0',
                # Mobile Chrome
                'Mozilla/5.0 (Linux; Android 13; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
                # iPhone Safari
                'Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1',
                # Very simple bot-like
                'curl/7.68.0',
                'Wget/1.21.2'
            ]
            
            last_error = None
            
            # áƒ§áƒ•áƒ”áƒšáƒ URL-áƒ¡áƒ áƒ“áƒ User-Agent-áƒ˜áƒ¡ áƒ™áƒáƒ›áƒ‘áƒ˜áƒœáƒáƒªáƒ˜áƒ˜áƒ¡ áƒªáƒ“áƒ
            for attempt_url in urls_to_try:
                for user_agent in user_agents:
                    try:
                        logger.info(f"ğŸ”„ áƒªáƒ“áƒ: {attempt_url} | UA: {user_agent[:50]}...")
                        
                        # Headers áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ£áƒ áƒáƒªáƒ˜áƒ
                        headers = {
                            'User-Agent': user_agent,
                            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                            'Accept-Language': 'en-US,en;q=0.5',
                            'Accept-Encoding': 'gzip, deflate, br',
                            'DNT': '1',
                            'Connection': 'keep-alive',
                            'Upgrade-Insecure-Requests': '1'
                        }
                        
                        # Cloudflare-friendly headers
                        if 'Chrome' in user_agent:
                            headers.update({
                                'Sec-Fetch-Dest': 'document',
                                'Sec-Fetch-Mode': 'navigate',
                                'Sec-Fetch-Site': 'none',
                                'Sec-Fetch-User': '?1',
                                'Cache-Control': 'max-age=0'
                            })
                        
                        # áƒ›áƒªáƒ“áƒ”áƒšáƒáƒ‘áƒ 1: áƒœáƒáƒ áƒ›áƒáƒšáƒ£áƒ áƒ˜ request
                        async with self.session.get(
                            attempt_url,
                            headers=headers,
                            allow_redirects=True,
                            max_redirects=10
                        ) as response:
                            
                            logger.info(f"ğŸ“Š Response: {response.status} | {attempt_url}")
                            
                            if response.status == 200:
                                content_type = response.headers.get('content-type', '').lower()
                                if 'html' in content_type or 'text' in content_type:
                                    content = await response.text(encoding='utf-8', errors='ignore')
                                    
                                    # áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒ•áƒáƒšáƒ˜áƒ“áƒáƒªáƒ˜áƒ
                                    if len(content.strip()) > 200:
                                        # Cloudflare áƒ áƒ”áƒ’áƒ˜áƒ¡áƒ¢áƒ áƒáƒªáƒ˜áƒ
                                        if 'cloudflare' in content.lower() and 'checking your browser' in content.lower():
                                            logger.warning(f"âš ï¸ Cloudflare áƒ áƒ”áƒ’áƒ˜áƒ¡áƒ¢áƒ áƒáƒªáƒ˜áƒ: {attempt_url}")
                                            continue
                                        
                                        # JavaScript redirect áƒ áƒ”áƒ’áƒ˜áƒ¡áƒ¢áƒ áƒáƒªáƒ˜áƒ
                                        if len(content) < 1000 and 'window.location' in content:
                                            logger.warning(f"âš ï¸ JS redirect: {attempt_url}")
                                            continue
                                        
                                        logger.info(f"âœ… áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ! {attempt_url} ({len(content)} chars)")
                                        return content
                                    else:
                                        logger.warning(f"âŒ áƒªáƒáƒ áƒ˜áƒ”áƒšáƒ˜ áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ˜: {attempt_url}")
                            
                            elif response.status in [301, 302, 303, 307, 308]:
                                redirect_url = str(response.url)
                                logger.info(f"â†ªï¸ Redirect: {redirect_url}")
                                
                                # Manual redirect follow
                                if redirect_url not in urls_to_try:
                                    async with self.session.get(redirect_url, headers=headers) as redir_resp:
                                        if redir_resp.status == 200:
                                            content = await redir_resp.text(encoding='utf-8', errors='ignore')
                                            if len(content.strip()) > 200:
                                                return content
                            
                            elif response.status == 403:
                                logger.warning(f"ğŸš« 403 Forbidden: {attempt_url}")
                                last_error = f"403 Forbidden"
                            
                            elif response.status == 404:
                                logger.warning(f"ğŸ” 404 Not Found: {attempt_url}")
                                last_error = f"404 Not Found"
                            
                            elif response.status >= 500:
                                logger.warning(f"ğŸ”¥ Server Error {response.status}: {attempt_url}")
                                last_error = f"Server Error {response.status}"
                                
                            else:
                                logger.warning(f"â“ HTTP {response.status}: {attempt_url}")
                                last_error = f"HTTP {response.status}"
                        
                        # Short delay between attempts
                        await asyncio.sleep(0.1)
                        
                    except aiohttp.ClientSSLError as ssl_error:
                        logger.warning(f"ğŸ”’ SSL error: {ssl_error}")
                        last_error = f"SSL Error"
                        continue
                        
                    except aiohttp.ClientConnectorError as conn_error:
                        logger.warning(f"ğŸ”Œ Connection error: {conn_error}")
                        last_error = f"Connection Error"
                        continue
                        
                    except asyncio.TimeoutError:
                        logger.warning(f"â° Timeout: {attempt_url}")
                        last_error = "Timeout"
                        continue
                        
                    except Exception as e:
                        logger.warning(f"âŒ Error: {str(e)[:100]}")
                        last_error = f"Error: {str(e)[:50]}"
                        continue
            
            # áƒ—áƒ£ áƒ§áƒ•áƒ”áƒšáƒ áƒ›áƒªáƒ“áƒ”áƒšáƒáƒ‘áƒ áƒ•áƒ”áƒ  áƒ¨áƒ”áƒ“áƒ’áƒ
            logger.error(f"ğŸ’¥ áƒ§áƒ•áƒ”áƒšáƒ áƒ›áƒªáƒ“áƒ”áƒšáƒáƒ‘áƒ áƒ•áƒ”áƒ  áƒ¨áƒ”áƒ“áƒ’áƒ! URLs: {len(urls_to_try)}, UAs: {len(user_agents)}")
            logger.error(f"ğŸ” áƒ‘áƒáƒšáƒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {last_error}")
            return None
            
        except Exception as e:
            logger.error(f"ğŸ’€ Critical error in fetch_website_content: {str(e)}")
            return None
    
    def parse_products(self, html_content, base_url):
        """HTML-áƒ˜áƒ¡ áƒáƒáƒ áƒ¡áƒ˜áƒœáƒ’áƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ›áƒáƒ¡áƒáƒáƒáƒ•áƒ”áƒ‘áƒšáƒáƒ“ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜"""
        soup = BeautifulSoup(html_content, 'html.parser')
        products = []
        
        # áƒ’áƒáƒ¤áƒáƒ áƒ—áƒáƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ¡áƒ”áƒšáƒ”áƒ¥áƒ¢áƒáƒ áƒ”áƒ‘áƒ˜
        product_selectors = [
            # áƒ¡áƒ¢áƒáƒœáƒ“áƒáƒ áƒ¢áƒ£áƒšáƒ˜ áƒ™áƒšáƒáƒ¡áƒ”áƒ‘áƒ˜
            '.product-item', '.product-card', '.item-product', '.product-container',
            '.card', '[data-product-id]', '.product-list-item', '.product',
            '.item', '[class*="product"]', '.item-card', '.product-tile',
            
            # e-commerce áƒáƒšáƒáƒ¢áƒ¤áƒáƒ áƒ›áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒáƒ”áƒªáƒ˜áƒ¤áƒ˜áƒ™áƒ£áƒ áƒ˜ áƒ™áƒšáƒáƒ¡áƒ”áƒ‘áƒ˜
            '.woocommerce-LoopProduct-link', '.product-inner', '.product-wrapper',
            '.product-box', '.shop-item', '.catalog-item', '.store-item',
            '.goods-item', '.merchandise-item', '.listing-item',
            
            # áƒ–áƒáƒ’áƒáƒ“áƒ˜ áƒ™áƒáƒœáƒ¢áƒ”áƒ˜áƒœáƒ”áƒ áƒ”áƒ‘áƒ˜
            '.grid-item', '.list-item', '.tile', '.card-body', '.item-wrapper',
            '.content-item', '.media', '.thumbnail', '.preview',
            
            # áƒ’áƒáƒšáƒ”áƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒ áƒ™áƒáƒ¢áƒáƒšáƒáƒ’áƒ”áƒ‘áƒ˜áƒ¡ áƒ™áƒšáƒáƒ¡áƒ”áƒ‘áƒ˜
            '.gallery-item', '.portfolio-item', '.showcase-item',
            'article[class*="item"]', 'article[class*="product"]',
            'div[itemtype*="Product"]', '[itemscope][itemtype*="Product"]'
        ]
        
        logger.info(f"áƒ•áƒáƒœáƒáƒšáƒ˜áƒ–áƒ”áƒ‘ HTML áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ¡ ({len(html_content)} áƒ¡áƒ˜áƒ›áƒ‘áƒáƒšáƒ)")
        
        for selector in product_selectors:
            try:
                items = soup.select(selector)
                logger.info(f"áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ {len(items)} áƒ”áƒšáƒ”áƒ›áƒ”áƒœáƒ¢áƒ˜ áƒ¡áƒ”áƒšáƒ”áƒ¥áƒ¢áƒáƒ áƒ˜áƒ—: {selector}")
                
                if items and len(items) > 1:  # áƒ›áƒ˜áƒœáƒ˜áƒ›áƒ£áƒ› 2 áƒ”áƒšáƒ”áƒ›áƒ”áƒœáƒ¢áƒ˜ áƒ£áƒœáƒ“áƒ áƒ˜áƒ§áƒáƒ¡
                    temp_products = []
                    for i, item in enumerate(items[:15]):  # áƒ›áƒáƒ¥áƒ¡áƒ˜áƒ›áƒ£áƒ› 15 áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜
                        product = self.extract_product_info(item, base_url)
                        if product and product not in temp_products:
                            temp_products.append(product)
                            
                    if len(temp_products) > 1:  # áƒ›áƒ˜áƒœáƒ˜áƒ›áƒ£áƒ› 2 áƒ•áƒáƒšáƒ˜áƒ“áƒ£áƒ áƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜
                        products = temp_products
                        logger.info(f"áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ {len(products)} áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜ áƒ¡áƒ”áƒšáƒ”áƒ¥áƒ¢áƒáƒ áƒ˜áƒ—: {selector}")
                        break
                        
            except Exception as e:
                logger.warning(f"áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ áƒ¡áƒ”áƒšáƒ”áƒ¥áƒ¢áƒáƒ áƒ—áƒáƒœ {selector}: {e}")
                continue
        
        # áƒ—áƒ£ áƒ™áƒáƒœáƒ™áƒ áƒ”áƒ¢áƒ£áƒšáƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ”áƒ‘áƒ˜ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ, áƒ–áƒáƒ’áƒáƒ“áƒ˜ áƒ«áƒ”áƒ‘áƒœáƒ
        if not products:
            logger.info("áƒ•áƒªáƒ“áƒ˜áƒšáƒáƒ‘ áƒ–áƒáƒ’áƒáƒ“ áƒ«áƒ”áƒ‘áƒœáƒáƒ¡...")
            fallback_selectors = [
                'div[class*="item"]', 'div[class*="card"]', 'div[class*="box"]',
                'div[class*="product"]', 'article', 'li[class*="item"]',
                'div[class*="listing"]', 'div[class*="grid"]', '.row > div',
                'div[class*="col"]', 'div[id*="product"]', 'div[data-*]'
            ]
            
            for selector in fallback_selectors:
                try:
                    items = soup.select(selector)
                    if items and len(items) >= 3:
                        temp_products = []
                        for item in items[:20]:
                            product = self.extract_product_info(item, base_url)
                            if product and self.is_valid_product(product) and product not in temp_products:
                                temp_products.append(product)
                                
                        if len(temp_products) >= 2:
                            products = temp_products
                            logger.info(f"fallback: áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ {len(products)} áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜")
                            break
                            
                except Exception as e:
                    continue
        
        # áƒ“áƒ£áƒ‘áƒšáƒ˜áƒ™áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ¬áƒáƒ¨áƒšáƒ
        unique_products = []
        seen_names = set()
        for product in products:
            name_key = product['name'].lower().strip()
            if name_key not in seen_names:
                seen_names.add(name_key)
                unique_products.append(product)
        
        logger.info(f"áƒ¤áƒ˜áƒœáƒáƒšáƒ£áƒ áƒ˜ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜: {len(unique_products)} áƒ£áƒœáƒ˜áƒ™áƒáƒšáƒ£áƒ áƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜")
        return unique_products[:8]  # áƒ›áƒáƒ¥áƒ¡áƒ˜áƒ›áƒ£áƒ› 8 áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜
    
    def is_valid_product(self, product):
        """áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜áƒ¡ áƒ•áƒáƒšáƒ˜áƒ“áƒ£áƒ áƒáƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ"""
        if not product or not isinstance(product, dict):
            return False
            
        name = product.get('name', '').strip()
        price = product.get('price', '').strip()
        
        # áƒ¡áƒáƒ®áƒ”áƒšáƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
        if not name or len(name) < 3:
            return False
            
        # áƒ«áƒáƒšáƒ˜áƒáƒœ áƒ®áƒ¨áƒ˜áƒ áƒ˜ áƒ¡áƒ˜áƒ¢áƒ§áƒ•áƒ”áƒ‘áƒ˜ áƒ áƒáƒ›áƒšáƒ”áƒ‘áƒ˜áƒª áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜áƒ¡ áƒ¡áƒáƒ®áƒ”áƒšáƒ”áƒ‘áƒ˜
        invalid_keywords = ['menu', 'navigation', 'footer', 'header', 'sidebar', 
                          'search', 'login', 'register', 'cart', 'checkout',
                          'contact', 'about', 'privacy', 'terms', 'cookie']
        
        if any(keyword in name.lower() for keyword in invalid_keywords):
            return False
            
        # áƒ«áƒáƒšáƒ˜áƒáƒœ áƒ›áƒáƒ™áƒšáƒ” áƒáƒœ áƒ«áƒáƒšáƒ˜áƒáƒœ áƒ’áƒ áƒ«áƒ”áƒšáƒ˜ áƒ¡áƒáƒ®áƒ”áƒšáƒ”áƒ‘áƒ˜
        if len(name) < 3 or len(name) > 200:
            return False
            
        return True
    
    def extract_product_info(self, item, base_url):
        """áƒªáƒáƒšáƒ™áƒ”áƒ£áƒšáƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜áƒ¡ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ˜áƒ¡ áƒáƒ›áƒáƒ¦áƒ”áƒ‘áƒ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜"""
        try:
            # áƒ¡áƒáƒ®áƒ”áƒšáƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ - áƒ’áƒáƒ¤áƒáƒ áƒ—áƒáƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ¡áƒ”áƒšáƒ”áƒ¥áƒ¢áƒáƒ áƒ”áƒ‘áƒ˜áƒ—
            name_selectors = [
                'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
                '.title', '.name', '.product-name', '.product-title',
                '.item-title', '.card-title', '.heading',
                'a[title]', '.link-title', '.post-title',
                '.product-info h1', '.product-info h2', '.product-info h3',
                '[itemprop="name"]', '.entry-title', '.article-title',
                '.content-title', '.main-title', '.primary-title'
            ]
            
            name = None
            for selector in name_selectors:
                try:
                    name_elem = item.select_one(selector)
                    if name_elem:
                        name_text = name_elem.get_text(strip=True)
                        if len(name_text) > 3 and len(name_text) < 150:
                            name = name_text
                            break
                        
                        # title áƒáƒ¢áƒ áƒ˜áƒ‘áƒ£áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
                        if not name and selector == 'a[title]':
                            title_attr = name_elem.get('title', '').strip()
                            if len(title_attr) > 3 and len(title_attr) < 150:
                                name = title_attr
                                break
                except Exception:
                    continue
            
            # áƒ¤áƒáƒ¡áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ pattern matching
            price_selectors = [
                '.price', '.cost', '[class*="price"]', '[class*="cost"]',
                '.amount', '.value', '.product-price', '.item-price',
                '[itemprop="price"]', '.price-current', '.price-now',
                '.sale-price', '.regular-price', '.final-price',
                '.currency', '.money', '.sum', '.total'
            ]
            
            price = None
            for selector in price_selectors:
                try:
                    price_elem = item.select_one(selector)
                    if price_elem:
                        price_text = price_elem.get_text(strip=True)
                        
                        # Georgian, USD, EUR áƒ¤áƒáƒ¡áƒ”áƒ‘áƒ˜áƒ¡ pattern-áƒ”áƒ‘áƒ˜
                        price_patterns = [
                            r'(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)\s*(?:â‚¾|áƒšáƒáƒ áƒ˜|GEL)',
                            r'(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)\s*(?:\$|USD|dollar)',
                            r'(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)\s*(?:â‚¬|EUR|euro)',
                            r'(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)\s*(?:â‚½|RUB|rub)',
                            r'â‚¾\s*(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)',
                            r'\$\s*(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)',
                            r'(\d{1,6}(?:\.\d{2})?)'  # áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ›áƒ˜áƒ”áƒ áƒ˜ áƒ áƒ˜áƒªáƒ®áƒ•áƒ˜
                        ]
                        
                        for pattern in price_patterns:
                            price_match = re.search(pattern, price_text.replace(' ', ''))
                            if price_match:
                                price_value = price_match.group(1).replace(',', '')
                                try:
                                    price_float = float(price_value)
                                    if 0.01 <= price_float <= 1000000:  # áƒ áƒ”áƒáƒšáƒ£áƒ  áƒ“áƒ˜áƒáƒáƒáƒ–áƒáƒœáƒ¨áƒ˜
                                        currency = 'â‚¾'  # default currency
                                        if any(symbol in price_text for symbol in ['$', 'USD']):
                                            currency = '$'
                                        elif any(symbol in price_text for symbol in ['â‚¬', 'EUR']):
                                            currency = 'â‚¬'
                                        elif any(symbol in price_text for symbol in ['â‚½', 'RUB']):
                                            currency = 'â‚½'
                                        
                                        price = f"{price_value}{currency}"
                                        break
                                except ValueError:
                                    continue
                        if price:
                            break
                except Exception:
                    continue
            
            # áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜
            image_url = None
            img_selectors = [
                'img', '.product-image img', '.image img', 
                '.photo img', '.thumbnail img', '.item-image img', '.card-img img',
                '.gallery img', '.preview img', '.media img', '.picture img'
            ]
            
            for selector in img_selectors:
                try:
                    img_elem = item.select_one(selector)
                    if img_elem:
                        img_src = (img_elem.get('src') or 
                                  img_elem.get('data-src') or 
                                  img_elem.get('data-lazy-src') or
                                  img_elem.get('data-original') or
                                  img_elem.get('data-srcset') or
                                  img_elem.get('srcset'))
                        
                        if img_src:
                            # srcset-áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒ—áƒ®áƒ•áƒ”áƒ•áƒáƒ¨áƒ˜ áƒáƒ˜áƒ áƒ•áƒ”áƒšáƒ˜ URL-áƒ˜áƒ¡ áƒáƒ¦áƒ”áƒ‘áƒ
                            if ',' in img_src:
                                img_src = img_src.split(',')[0].split(' ')[0]
                            
                            # URL-áƒ˜áƒ¡ áƒœáƒáƒ áƒ›áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ
                            if img_src.startswith('//'):
                                img_src = 'https:' + img_src
                            elif img_src.startswith('/'):
                                img_src = urljoin(base_url, img_src)
                            elif not img_src.startswith('http'):
                                img_src = urljoin(base_url, img_src)
                            
                            if self.is_valid_image_url(img_src):
                                image_url = img_src
                                break
                except Exception:
                    continue
            
            # background-image áƒ«áƒ”áƒ‘áƒœáƒ
            if not image_url:
                bg_selectors = ['.product-image', '.image', '.photo', '.thumbnail', '.item-image', '.card-img']
                for selector in bg_selectors:
                    try:
                        bg_elem = item.select_one(selector)
                        if bg_elem:
                            style = bg_elem.get('style', '')
                            bg_match = re.search(r'background-image:\s*url\(["\']?(.*?)["\']?\)', style)
                            if bg_match:
                                bg_url = bg_match.group(1)
                                if bg_url.startswith('//'):
                                    bg_url = 'https:' + bg_url
                                elif bg_url.startswith('/'):
                                    bg_url = urljoin(base_url, bg_url)
                                
                                if self.is_valid_image_url(bg_url):
                                    image_url = bg_url
                                    break
                    except Exception:
                        continue
            
            # áƒšáƒ˜áƒœáƒ™áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ
            link_url = None
            try:
                link_elem = item.select_one('a')
                if link_elem:
                    href = link_elem.get('href')
                    if href:
                        if href.startswith('//'):
                            href = 'https:' + href
                        elif href.startswith('/'):
                            href = urljoin(base_url, href)
                        elif not href.startswith('http'):
                            href = urljoin(base_url, href)
                        link_url = href
            except Exception:
                pass
            
            # áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜áƒ¡ áƒ“áƒáƒ‘áƒ áƒ£áƒœáƒ”áƒ‘áƒ
            if name and (price or image_url):  # áƒ¡áƒáƒ®áƒ”áƒšáƒ˜ áƒ“áƒ (áƒ¤áƒáƒ¡áƒ˜ áƒáƒœ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜) áƒ›áƒáƒ˜áƒœáƒª áƒ£áƒœáƒ“áƒ áƒ˜áƒ§áƒáƒ¡
                return {
                    'name': name[:150],
                    'price': price or 'áƒ¤áƒáƒ¡áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ›áƒ˜áƒ—áƒ˜áƒ—áƒ”áƒ‘áƒ£áƒšáƒ˜',
                    'image_url': image_url,
                    'link_url': link_url
                }
                
        except Exception as e:
            logger.warning(f"áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜áƒ¡ áƒáƒ›áƒáƒ¦áƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {str(e)}")
        
        return None
    
    def is_valid_image_url(self, url):
        """áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ URL-áƒ˜áƒ¡ áƒ•áƒáƒšáƒ˜áƒ“áƒáƒªáƒ˜áƒ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜"""
        if not url or len(url) < 10:
            return False
        
        # áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ¤áƒáƒ˜áƒšáƒ˜áƒ¡ áƒ’áƒáƒ¤áƒáƒ áƒ—áƒáƒ”áƒ‘áƒ”áƒ‘áƒ˜
        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp', '.svg', '.avif', '.jfif']
        url_lower = url.lower()
        
        # áƒáƒ˜áƒ áƒ“áƒáƒáƒ˜áƒ áƒ˜ áƒ’áƒáƒ¤áƒáƒ áƒ—áƒáƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
        if any(url_lower.endswith(ext) for ext in image_extensions):
            return True
        
        # URL-áƒ¨áƒ˜ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ˜áƒœáƒ“áƒ˜áƒ™áƒáƒ¢áƒáƒ áƒ”áƒ‘áƒ˜
        image_indicators = ['image', 'img', 'photo', 'picture', 'pic', 'thumb', 'thumbnail', 
                           'avatar', 'logo', 'banner', 'gallery', 'media', 'asset']
        if any(indicator in url_lower for indicator in image_indicators):
            return True
        
        # Data URL
        if url.startswith('data:image'):
            return True
        
        # CDN áƒ“áƒ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ¡áƒ”áƒ áƒ•áƒ˜áƒ¡áƒ”áƒ‘áƒ˜
        image_services = ['imgur', 'cloudinary', 'unsplash', 'pixabay', 'pexels', 
                         'shutterstock', 'getty', 'istockphoto']
        if any(service in url_lower for service in image_services):
            return True
            
        return False

    async def send_products_with_images(self, update, products, website_name="áƒ¡áƒáƒ˜áƒ¢áƒ˜"):
        """áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ’áƒáƒ’áƒ–áƒáƒ•áƒœáƒ áƒ¡áƒ£áƒ áƒáƒ—áƒ”áƒ‘áƒ˜áƒ— - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜"""
        if not products:
            await update.message.reply_text("ğŸš« áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ áƒáƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ áƒáƒ› áƒ¡áƒáƒ˜áƒ¢áƒ–áƒ”.")
            return
        
        limited_products = products[:6]  # áƒ›áƒáƒ¥áƒ¡áƒ˜áƒ›áƒ£áƒ› 6 áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜
        
        # áƒ¨áƒ”áƒ¡áƒáƒ•áƒáƒšáƒ˜ áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ
        intro_message = f"ğŸ›ï¸ *áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ {len(limited_products)} áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜ áƒ¡áƒáƒ˜áƒ¢áƒ–áƒ”:* {website_name}\n\n"
        await update.message.reply_text(intro_message, parse_mode='Markdown')
        
        for i, product in enumerate(limited_products, 1):
            try:
                # áƒ™áƒáƒáƒ¢áƒ˜áƒáƒœáƒ˜áƒ¡ áƒ¤áƒáƒ áƒ›áƒ˜áƒ áƒ”áƒ‘áƒ
                caption = f"*{i}. {product['name']}*\n\n"
                caption += f"ğŸ’° *áƒ¤áƒáƒ¡áƒ˜:* `{product['price']}`\n"
                
                if product.get('link_url'):
                    caption += f"ğŸ”— [áƒ¡áƒ áƒ£áƒšáƒ˜ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ â†’]({product['link_url']})\n"
                
                caption += f"\nğŸ“Š *áƒ¬áƒ§áƒáƒ áƒ:* {website_name}"
                
                # áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ’áƒáƒ’áƒ–áƒáƒ•áƒœáƒ
                if product.get('image_url') and self.is_valid_image_url(product['image_url']):
                    try:
                        await update.message.reply_photo(
                            photo=product['image_url'],
                            caption=caption,
                            parse_mode='Markdown'
                        )
                        logger.info(f"áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ— áƒ’áƒáƒ˜áƒ’áƒ–áƒáƒ•áƒœáƒ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡: {product['name']}")
                    except Exception as img_error:
                        logger.warning(f"áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ’áƒáƒ’áƒ–áƒáƒ•áƒœáƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ {product['name']}: {img_error}")
                        # áƒ¡áƒ£áƒ áƒáƒ—áƒ˜áƒ¡ áƒ’áƒáƒ áƒ”áƒ¨áƒ” áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜
                        await update.message.reply_text(
                            f"ğŸ“¦ {caption}\n\nâŒ áƒ¡áƒ£áƒ áƒáƒ—áƒ˜ áƒ•áƒ”áƒ  áƒ©áƒáƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ",
                            parse_mode='Markdown',
                            disable_web_page_preview=False
                        )
                else:
                    # áƒ›áƒ®áƒáƒšáƒáƒ“ áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜
                    await update.message.reply_text(
                        f"ğŸ“¦ {caption}",
                        parse_mode='Markdown',
                        disable_web_page_preview=False
                    )
                    
                # Rate limiting - Telegram-áƒ˜áƒ¡ áƒšáƒ˜áƒ›áƒ˜áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ—áƒáƒ•áƒ˜áƒ“áƒáƒœ áƒáƒ áƒ˜áƒ“áƒ”áƒ‘áƒ
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.error(f"áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ˜áƒ¡ {i} áƒ’áƒáƒ’áƒ–áƒáƒ•áƒœáƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ: {str(e)}")
                continue

    async def check_ssl_certificate(self, url):
        """SSL áƒ¡áƒ”áƒ áƒ¢áƒ˜áƒ¤áƒ˜áƒ™áƒáƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜"""
        try:
            parsed_url = urlparse(url)
            if parsed_url.scheme != 'https':
                return True  # HTTP-áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡ SSL áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ¡áƒáƒ­áƒ˜áƒ áƒ
            
            context = ssl.create_default_context()
            
            with socket.create_connection((parsed_url.hostname, 443), timeout=10) as sock:
                with context.wrap_socket(sock, server_hostname=parsed_url.hostname) as ssock:
                    cert = ssock.getpeercert()
                    if cert:
                        logger.info(f"SSL áƒ¡áƒ”áƒ áƒ¢áƒ˜áƒ¤áƒ˜áƒ™áƒáƒ¢áƒ˜ áƒ•áƒáƒšáƒ˜áƒ“áƒ£áƒ áƒ˜áƒ {parsed_url.hostname}-áƒ¡áƒ—áƒ•áƒ˜áƒ¡")
                        return True
            return False
            
        except Exception as e:
            logger.warning(f"SSL áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ {url}: {e}")
            return False

class TelegramBot:
    def __init__(self, bot_token):
        self.bot_token = bot_token
        self.product_bot = ProductBot(bot_token)
        
    async def test_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Test áƒ™áƒáƒ›áƒáƒœáƒ“áƒ - áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ¡áƒ áƒ£áƒšáƒ˜ áƒ“áƒ˜áƒáƒ’áƒœáƒáƒ¡áƒ¢áƒ˜áƒ™áƒ"""
        if not context.args:
            await update.message.reply_text(
                "ğŸ§ª *áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ¢áƒ”áƒ¡áƒ¢áƒ˜*\n\n"
                "áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ: `/test <URL>`\n\n"
                "áƒ›áƒáƒ’áƒáƒšáƒ˜áƒ—áƒ˜: `/test google.com`\n\n"
                "áƒ”áƒ¡ áƒ™áƒáƒ›áƒáƒœáƒ“áƒ áƒ’áƒáƒáƒœáƒáƒšáƒ˜áƒ–áƒ”áƒ‘áƒ¡:\n"
                "â€¢ DNS resolution\n"
                "â€¢ SSL connection\n"
                "â€¢ HTTP response\n"
                "â€¢ Content analysis\n"
                "â€¢ Product detection",
                parse_mode='Markdown'
            )
            return
        
        url = ' '.join(context.args)
        test_message = await update.message.reply_text("ğŸ§ª áƒ˜áƒ¬áƒ§áƒ”áƒ‘áƒ áƒ¡áƒ áƒ£áƒšáƒ˜ áƒ¢áƒ”áƒ¡áƒ¢áƒ˜...")
        
        try:
            # áƒ¡áƒ”áƒ¡áƒ˜áƒ˜áƒ¡ áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ
            if not self.product_bot.session or self.product_bot.session.closed:
                await self.product_bot.init_session()
            
            results = {}
            
            # 1. URL áƒœáƒáƒ áƒ›áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ
            await test_message.edit_text("ğŸ§ª 1/6 URL áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜...")
            normalized_url = self.product_bot.normalize_url(url)
            parsed = urlparse(normalized_url)
            results['url_analysis'] = {
                'original': url,
                'normalized': normalized_url,
                'domain': parsed.netloc,
                'scheme': parsed.scheme
            }
            
            # 2. DNS Resolution
            await test_message.edit_text("ğŸ§ª 2/6 DNS Resolution...")
            try:
                import socket
                ip = socket.gethostbyname(parsed.netloc.replace('www.', ''))
                results['dns'] = {'status': 'âœ…', 'ip': ip}
            except Exception as e:
                results['dns'] = {'status': 'âŒ', 'error': str(e)[:50]}
            
            # 3. SSL Test
            await test_message.edit_text("ğŸ§ª 3/6 SSL Test...")
            if normalized_url.startswith('https://'):
                ssl_result = await self.product_bot.check_ssl_certificate(normalized_url)
                results['ssl'] = {'status': 'âœ…' if ssl_result else 'âš ï¸'}
            else:
                results['ssl'] = {'status': 'ğŸ”“', 'note': 'HTTP connection'}
            
            # 4. HTTP Connection Test
            await test_message.edit_text("ğŸ§ª 4/6 HTTP Connection...")
            try:
                async with self.product_bot.session.get(
                    normalized_url,
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as response:
                    results['http'] = {
                        'status': f"âœ… {response.status}",
                        'headers': dict(list(response.headers.items())[:5])
                    }
                    content = await response.text(encoding='utf-8', errors='ignore')
                    results['content_length'] = len(content)
            except Exception as e:
                results['http'] = {'status': 'âŒ', 'error': str(e)[:50]}
                content = ""
            
            # 5. Content Analysis
            await test_message.edit_text("ğŸ§ª 5/6 Content Analysis...")
            if content:
                content_analysis = {
                    'length': len(content),
                    'has_html': '<html' in content.lower(),
                    'has_body': '<body' in content.lower(),
                    'title_found': bool(re.search(r'<title[^>]*>([^<]+)</title>', content, re.IGNORECASE)),
                    'ecommerce_keywords': sum(1 for word in ['product', 'price', 'buy', 'cart', 'shop', 'store'] if word in content.lower()),
                    'encoding_issues': content.count('ï¿½')
                }
                
                # Title extraction
                title_match = re.search(r'<title[^>]*>([^<]+)</title>', content, re.IGNORECASE)
                content_analysis['title'] = title_match.group(1).strip() if title_match else "Title not found"
                
                results['content_analysis'] = content_analysis
                
                # 6. Product Detection
                await test_message.edit_text("ğŸ§ª 6/6 Product Detection...")
                products = self.product_bot.parse_products(content, normalized_url)
                results['products'] = {
                    'found': len(products),
                    'details': [{'name': p['name'][:30] + '...', 'price': p['price']} for p in products[:3]]
                }
            else:
                results['content_analysis'] = {'error': 'No content received'}
                results['products'] = {'found': 0, 'error': 'No content to analyze'}
            
            # áƒ¤áƒ˜áƒœáƒáƒšáƒ£áƒ áƒ˜ áƒ áƒ”áƒáƒáƒ áƒ¢áƒ˜
            await test_message.delete()
            
            report = f"ğŸ§ª **áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ¡áƒ áƒ£áƒšáƒ˜ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜**\n\n"
            report += f"ğŸŒ **URL áƒ˜áƒœáƒ¤áƒ:**\n"
            report += f"â€¢ áƒáƒ áƒ˜áƒ’áƒ˜áƒœáƒáƒšáƒ˜: `{results['url_analysis']['original']}`\n"
            report += f"â€¢ áƒœáƒáƒ áƒ›áƒáƒšáƒ˜áƒ–áƒ”áƒ‘áƒ£áƒšáƒ˜: `{results['url_analysis']['normalized']}`\n"
            report += f"â€¢ áƒ“áƒáƒ›áƒ”áƒœáƒ˜: `{results['url_analysis']['domain']}`\n\n"
            
            report += f"ğŸ“¡ **DNS:** {results['dns']['status']}"
            if 'ip' in results['dns']:
                report += f" (`{results['dns']['ip']}`)"
            elif 'error' in results['dns']:
                report += f" - {results['dns']['error']}"
            report += "\n\n"
            
            report += f"ğŸ”’ **SSL:** {results['ssl']['status']}"
            if 'note' in results['ssl']:
                report += f" - {results['ssl']['note']}"
            report += "\n\n"
            
            report += f"ğŸŒ **HTTP:** {results['http']['status']}\n"
            if 'error' in results['http']:
                report += f"Error: `{results['http']['error']}`\n"
            report += "\n"
            
            if 'content_analysis' in results and 'error' not in results['content_analysis']:
                ca = results['content_analysis']
                report += f"ğŸ“„ **Content Analysis:**\n"
                report += f"â€¢ Length: {ca['length']:,} characters\n"
                report += f"â€¢ HTML structure: {'âœ…' if ca['has_html'] else 'âŒ'}\n"
                report += f"â€¢ Title: {ca['title'][:50]}...\n"
                report += f"â€¢ E-commerce signals: {ca['ecommerce_keywords']}/6\n"
                if ca['encoding_issues'] > 0:
                    report += f"â€¢ âš ï¸ Encoding issues: {ca['encoding_issues']}\n"
                report += "\n"
            
            if 'products' in results:
                report += f"ğŸ›ï¸ **Product Detection:**\n"
                if results['products']['found'] > 0:
                    report += f"â€¢ Found: {results['products']['found']} products âœ…\n"
                    for product in results['products']['details']:
                        report += f"  - {product['name']} | {product['price']}\n"
                else:
                    report += f"â€¢ Found: 0 products âŒ\n"
                    if 'error' in results['products']:
                        report += f"  Error: {results['products']['error']}\n"
                report += "\n"
            
            # áƒ áƒ”áƒ™áƒáƒ›áƒ”áƒœáƒ“áƒáƒªáƒ˜áƒ”áƒ‘áƒ˜
            report += f"ğŸ’¡ **áƒ áƒ”áƒ™áƒáƒ›áƒ”áƒœáƒ“áƒáƒªáƒ˜áƒ”áƒ‘áƒ˜:**\n"
            if results['dns']['status'] == 'âŒ':
                report += f"â€¢ áƒ¨áƒ”áƒáƒ›áƒáƒ¬áƒ›áƒ”áƒ— áƒ“áƒáƒ›áƒ”áƒœáƒ˜áƒ¡ áƒ¡áƒ¬áƒáƒ áƒáƒ‘áƒ\n"
            elif results['http']['status'].startswith('âŒ'):
                report += f"â€¢ áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒ¨áƒ”áƒ¡áƒáƒ«áƒšáƒáƒ áƒ“áƒáƒ¥áƒ•áƒ”áƒ›áƒ“áƒ”áƒ‘áƒáƒ áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ˜áƒ§áƒáƒ¡\n"
            elif results.get('products', {}).get('found', 0) == 0:
                if results.get('content_analysis', {}).get('ecommerce_keywords', 0) > 0:
                    report += f"â€¢ áƒ¡áƒáƒ˜áƒ¢áƒ¡ áƒáƒ¥áƒ•áƒ¡ e-commerce áƒ”áƒšáƒ”áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜, áƒ›áƒáƒ’áƒ áƒáƒ› áƒáƒ áƒáƒ¡áƒ¢áƒáƒœáƒ“áƒáƒ áƒ¢áƒ£áƒšáƒ˜ áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒ\n"
                else:
                    report += f"â€¢ áƒ”áƒ¡ áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒáƒ  áƒ©áƒáƒœáƒ¡ áƒáƒœáƒšáƒáƒ˜áƒœ áƒ›áƒáƒ¦áƒáƒ–áƒ˜áƒáƒ“\n"
            else:
                report += f"â€¢ áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒ›áƒ–áƒáƒ“ áƒáƒ áƒ˜áƒ¡ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡! âœ…\n"
            
            await update.message.reply_text(report, parse_mode='Markdown')
            
        except Exception as e:
            logger.error(f"Test command error: {str(e)}")
            await test_message.edit_text(
                f"âŒ áƒ¢áƒ”áƒ¡áƒ¢áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ:\n`{str(e)[:200]}`"
            )
    
    async def start_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Start áƒ™áƒáƒ›áƒáƒœáƒ“áƒ"""
        keyboard = [
            [InlineKeyboardButton("ğŸ›’ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ", callback_data='search_products')],
            [InlineKeyboardButton("ğŸ§ª áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ¢áƒ”áƒ¡áƒ¢áƒ˜", callback_data='test_site')],
            [InlineKeyboardButton("â„¹ï¸ áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ", callback_data='help')]
        ]
        reply_markup = InlineKeyboardMarkup(keyboard)
        
        welcome_text = (
            "ğŸ¤– **áƒ›áƒáƒ’áƒ”áƒ¡áƒáƒšáƒ›áƒ”áƒ‘áƒ˜áƒ—!**\n\n"
            "áƒ”áƒ¡ áƒ‘áƒáƒ¢áƒ˜ áƒ“áƒáƒ’áƒ”áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒáƒ— áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ›áƒ˜áƒ”áƒ áƒ˜ áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ“áƒáƒœ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ›áƒáƒ«áƒ”áƒ‘áƒœáƒáƒ¨áƒ˜.\n\n"
            "ğŸ“ **áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ”áƒ‘áƒ˜:**\n"
            "â€¢ ğŸ›’ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ›áƒ˜áƒ”áƒ  áƒ¡áƒáƒ˜áƒ¢áƒ–áƒ”\n"
            "â€¢ ğŸ§ª áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ¡áƒ áƒ£áƒšáƒ˜ áƒ¢áƒ”áƒ¥áƒœáƒ˜áƒ™áƒ£áƒ áƒ˜ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜\n"
            "â€¢ ğŸ” 9 áƒ¡áƒ®áƒ•áƒáƒ“áƒáƒ¡áƒ®áƒ•áƒ User-Agent\n"
            "â€¢ ğŸŒ HTTP/HTTPS áƒ›áƒ®áƒáƒ áƒ“áƒáƒ­áƒ”áƒ áƒ\n"
            "â€¢ ğŸ”’ SSL/TLS compatibility\n\n"
            "ğŸš€ **áƒáƒ®áƒáƒšáƒ˜ áƒ•áƒ”áƒ áƒ¡áƒ˜áƒ v2.0 - áƒ§áƒ•áƒ”áƒšáƒáƒ–áƒ” áƒ«áƒšáƒ˜áƒ”áƒ áƒ˜ parser!**\n\n"
            "áƒáƒ˜áƒ áƒ©áƒ˜áƒ”áƒ— áƒ¡áƒáƒ¡áƒ£áƒ áƒ•áƒ”áƒšáƒ˜ áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ:"
        )
        
        await update.message.reply_text(welcome_text, reply_markup=reply_markup, parse_mode='Markdown')
    
    async def search_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Search áƒ™áƒáƒ›áƒáƒœáƒ“áƒ"""
        if not context.args:
            await update.message.reply_text(
                "â— áƒ’áƒ—áƒ®áƒáƒ•áƒ— áƒ›áƒ˜áƒ£áƒ—áƒ˜áƒ—áƒáƒ— áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ URL\n\n"
                "âœ… *áƒ¡áƒ¬áƒáƒ áƒ˜ áƒ¤áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜:*\n"
                "â€¢ `/search https://example.com`\n"
                "â€¢ `/search example.com`\n"
                "â€¢ `/search shop.example.com`", 
                parse_mode='Markdown'
            )
            return
        
        url = ' '.join(context.args)  # URL-áƒ¨áƒ˜ áƒ¨áƒ”áƒ˜áƒ«áƒšáƒ”áƒ‘áƒ áƒ¡áƒ˜áƒ•áƒ áƒªáƒ”áƒ”áƒ‘áƒ˜ áƒ˜áƒ§áƒáƒ¡
        await self.process_website(update, url)
    
    async def handle_url_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """URL áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜"""
        text = update.message.text.strip()
        
        # URL-áƒ˜áƒ¡ áƒáƒ¦áƒ›áƒáƒ©áƒ”áƒœáƒ˜áƒ¡ áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜ regex
        url_patterns = [
            r'https?://[^\s]+',  # áƒ¡áƒ áƒ£áƒšáƒ˜ URL
            r'www\.[^\s]+',      # www-áƒ˜áƒáƒœáƒ˜
            r'[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}[^\s]*'  # áƒ“áƒáƒ›áƒ”áƒœáƒ˜
        ]
        
        found_url = None
        for pattern in url_patterns:
            urls = re.findall(pattern, text)
            if urls:
                found_url = urls[0]
                break
        
        if found_url:
            await self.process_website(update, found_url)
        else:
            # áƒ—áƒ£ URL áƒáƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ, áƒ›áƒáƒ’áƒ áƒáƒ› áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ˜ áƒ“áƒáƒ›áƒ”áƒœáƒ˜áƒ¡ áƒ›áƒ¡áƒ’áƒáƒ•áƒ¡áƒ˜áƒ
            if '.' in text and len(text.split('.')) >= 2 and len(text) < 100:
                await self.process_website(update, text)
            else:
                await update.message.reply_text(
                    "â— áƒ’áƒ—áƒ®áƒáƒ•áƒ— áƒ’áƒáƒ›áƒáƒáƒ’áƒ–áƒáƒ•áƒœáƒáƒ— áƒ•áƒáƒšáƒ˜áƒ“áƒ£áƒ áƒ˜ URL\n\n"
                    "âœ… *áƒ›áƒáƒ’áƒáƒšáƒ˜áƒ—áƒ”áƒ‘áƒ˜:*\n"
                    "â€¢ `https://shop.example.com`\n"
                    "â€¢ `www.store.example.com`\n"
                    "â€¢ `example.com`"
                )
    
    async def process_website(self, update, url):
        """áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ“áƒáƒ›áƒ£áƒ¨áƒáƒ•áƒ”áƒ‘áƒ - áƒ«áƒáƒšáƒ˜áƒáƒœ áƒ“áƒ”áƒ¢áƒáƒšáƒ£áƒ áƒ˜ debugging"""
        original_url = url
        
        try:
            # URL-áƒ˜áƒ¡ áƒ•áƒáƒšáƒ˜áƒ“áƒáƒªáƒ˜áƒ
            url = url.strip()
            
            if not url or len(url) < 4:
                await update.message.reply_text("â— áƒ«áƒáƒšáƒ˜áƒáƒœ áƒ›áƒáƒ™áƒšáƒ” URL")
                return
                
            if ' ' in url and not url.startswith('http'):
                await update.message.reply_text("â— URL áƒáƒ  áƒ£áƒœáƒ“áƒ áƒ¨áƒ”áƒ˜áƒªáƒáƒ•áƒ“áƒ”áƒ¡ áƒ¡áƒ˜áƒ•áƒ áƒªáƒ”áƒ”áƒ‘áƒ¡")
                return
            
            # URL parsing áƒ¨áƒ”áƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ
            try:
                if not url.startswith(('http://', 'https://')):
                    test_url = 'https://' + url
                else:
                    test_url = url
                    
                parsed = urlparse(test_url)
                if not parsed.netloc or '.' not in parsed.netloc:
                    await update.message.reply_text("â— áƒáƒ áƒáƒ¡áƒ¬áƒáƒ áƒ˜ URL áƒ¤áƒáƒ áƒ›áƒáƒ¢áƒ˜")
                    return
                    
            except Exception:
                await update.message.reply_text("â— URL-áƒ˜áƒ¡ áƒáƒáƒ áƒ¡áƒ˜áƒœáƒ’áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ")
                return
            
        except Exception:
            await update.message.reply_text("â— URL-áƒ˜áƒ¡ áƒ•áƒáƒšáƒ˜áƒ“áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ")
            return
        
        # áƒáƒ áƒáƒ’áƒ áƒ”áƒ¡ áƒ›áƒ”áƒ¡áƒ˜áƒ¯áƒ”áƒ‘áƒ˜
        search_message = await update.message.reply_text("ğŸ” áƒ•áƒ˜áƒ¬áƒ§áƒ”áƒ‘ áƒ«áƒ”áƒ‘áƒœáƒáƒ¡...")
        
        try:
            # áƒ¡áƒ”áƒ¡áƒ˜áƒ˜áƒ¡ áƒ˜áƒœáƒ˜áƒªáƒ˜áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ
            if not self.product_bot.session or self.product_bot.session.closed:
                await search_message.edit_text("ğŸ”„ áƒ•áƒáƒ›áƒ–áƒáƒ“áƒ”áƒ‘ áƒ™áƒáƒ•áƒ¨áƒ˜áƒ áƒ¡...")
                await self.product_bot.init_session()
            
            await search_message.edit_text("ğŸŒ áƒ•áƒªáƒ“áƒ˜áƒšáƒáƒ‘ áƒ¡áƒáƒ˜áƒ¢áƒ–áƒ” áƒ“áƒáƒ™áƒáƒ•áƒ¨áƒ˜áƒ áƒ”áƒ‘áƒáƒ¡...")
            
            # áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒ
            html_content = await self.product_bot.fetch_website_content(url)
            
            if not html_content:
                await search_message.edit_text("âŒ áƒ¡áƒáƒ¢áƒ”áƒ¡áƒ¢áƒ: DNS resolution...")
                
                # DNS resolution test
                try:
                    parsed = urlparse(url if url.startswith(('http://', 'https://')) else f'https://{url}')
                    import socket
                    socket.gethostbyname(parsed.netloc.replace('www.', ''))
                    dns_status = "âœ… DNS OK"
                except:
                    dns_status = "âŒ DNS FAILED"
                
                # Ping test simulation
                try:
                    async with self.product_bot.session.get(
                        f"https://{parsed.netloc.replace('www.', '')}",
                        timeout=aiohttp.ClientTimeout(total=5)
                    ) as ping_response:
                        ping_status = f"âœ… PING OK ({ping_response.status})"
                except:
                    ping_status = "âŒ PING FAILED"
                
                await search_message.edit_text(
                    f"ğŸ” *áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ“áƒ˜áƒáƒ’áƒœáƒáƒ¡áƒ¢áƒ˜áƒ™áƒ:*\n\n"
                    f"ğŸŒ *URL:* `{original_url}`\n"
                    f"ğŸ“¡ *DNS:* {dns_status}\n"
                    f"ğŸ“ *Connection:* {ping_status}\n\n"
                    f"âŒ *áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ:* áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ®áƒ”áƒšáƒ›áƒ˜áƒ¡áƒáƒ¬áƒ•áƒ“áƒáƒ›áƒ˜\n\n"
                    f"ğŸ’¡ *áƒ¨áƒ”áƒ¡áƒáƒ«áƒšáƒ áƒ›áƒ˜áƒ–áƒ”áƒ–áƒ”áƒ‘áƒ˜:*\n"
                    f"â€¢ áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒ“áƒáƒ¥áƒ•áƒ”áƒ›áƒ“áƒ”áƒ‘áƒáƒ áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ\n"
                    f"â€¢ Cloudflare/DDoS protection\n"
                    f"â€¢ Geo-blocking\n"
                    f"â€¢ áƒáƒ áƒáƒ¡áƒ¬áƒáƒ áƒ˜ URL\n\n"
                    f"ğŸ”„ áƒ¡áƒªáƒáƒ“áƒ”áƒ— áƒ›áƒáƒ’áƒ•áƒ˜áƒáƒœáƒ”áƒ‘áƒ˜áƒ— áƒáƒœ áƒ¡áƒ®áƒ•áƒ URL",
                    parse_mode='Markdown'
                )
                return
            
            # áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜
            await search_message.edit_text("ğŸ§  áƒ•áƒáƒœáƒáƒšáƒ˜áƒ–áƒ”áƒ‘ áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ¡...")
            
            # áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ˜áƒ¡ áƒ¡áƒ¢áƒáƒ¢áƒ˜áƒ¡áƒ¢áƒ˜áƒ™áƒ
            content_stats = {
                'length': len(html_content),
                'has_html': '<html' in html_content.lower(),
                'has_body': '<body' in html_content.lower(),
                'has_title': '<title' in html_content.lower(),
                'has_products': any(word in html_content.lower() for word in ['product', 'price', 'shop', 'buy', 'cart']),
                'encoding_issues': html_content.count('ï¿½') > 5
            }
            
            logger.info(f"ğŸ“Š Content stats: {content_stats}")
            
            # áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ
            products = self.product_bot.parse_products(html_content, url)
            
            await search_message.delete()
            
            # áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜áƒ¡ áƒ©áƒ•áƒ”áƒœáƒ”áƒ‘áƒ
            if products:
                website_name = f"ğŸŒ {urlparse(url).netloc}"
                await self.product_bot.send_products_with_images(update, products, website_name)
            else:
                # áƒ—áƒ£ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ
                await update.message.reply_text(
                    f"ğŸ” *áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜áƒ¡ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜:*\n\n"
                    f"âœ… áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ— áƒ©áƒáƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ\n"
                    f"âŒ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ\n\n"
                    f"ğŸ¤” *áƒ¨áƒ”áƒ¡áƒáƒ«áƒšáƒ áƒ›áƒ˜áƒ–áƒ”áƒ–áƒ”áƒ‘áƒ˜:*\n"
                    f"â€¢ áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒáƒœáƒšáƒáƒ˜áƒœ áƒ›áƒáƒ¦áƒáƒ–áƒ˜áƒ\n"
                    f"â€¢ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ áƒ¡áƒáƒ”áƒªáƒ˜áƒáƒšáƒ£áƒ  áƒ¤áƒáƒ áƒ›áƒáƒ¢áƒ¨áƒ˜áƒ\n"
                    f"â€¢ áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ¡ JavaScript-áƒ¡ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ©áƒ•áƒ”áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡\n\n"
                    f"ğŸ’¡ áƒ¡áƒªáƒáƒ“áƒ”áƒ— áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ™áƒáƒ¢áƒ”áƒ’áƒáƒ áƒ˜áƒ˜áƒ¡ áƒ’áƒ•áƒ”áƒ áƒ“áƒ˜",
                    parse_mode='Markdown'
                )
            
        except asyncio.TimeoutError:
            await search_message.edit_text("â° áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒáƒ¡ áƒ“áƒ áƒ áƒáƒ›áƒáƒ£áƒ•áƒ˜áƒ“áƒ")
        except Exception as e:
            logger.error(f"Website processing error: {str(e)}")
            await search_message.edit_text(
                f"âŒ áƒ›áƒáƒ®áƒ“áƒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ\n\n"
                f"ğŸ”§ *áƒ¢áƒ”áƒ¥áƒœáƒ˜áƒ™áƒ£áƒ áƒ˜ áƒ“áƒ”áƒ¢áƒáƒšáƒ”áƒ‘áƒ˜:*\n"
                f"`{str(e)[:100]}...`"
            )
        finally:
            # áƒ áƒ”áƒ¡áƒ£áƒ áƒ¡áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ¬áƒ›áƒ”áƒœáƒ“áƒ
            if hasattr(self.product_bot, 'session') and self.product_bot.session:
                try:
                    # áƒáƒ  áƒ•áƒ®áƒ£áƒ áƒáƒ— áƒ¡áƒ”áƒ¡áƒ˜áƒ, áƒ˜áƒ’áƒ˜ áƒ¨áƒ”áƒ›áƒ“áƒ’áƒáƒ› áƒ’áƒáƒ›áƒáƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ
                    pass
                except Exception:
                    pass self.product_bot.parse_products(html_content, url)
            
            await search_message.delete()
            
            # áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜áƒ¡ áƒ©áƒ•áƒ”áƒœáƒ”áƒ‘áƒ
            if products:
                website_name = f"ğŸŒ {urlparse(url if url.startswith(('http://', 'https://')) else f'https://{url}').netloc}"
                await self.product_bot.send_products_with_images(update, products, website_name)
            else:
                # áƒ“áƒ”áƒ¢áƒáƒšáƒ£áƒ áƒ˜ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜áƒ¡ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜
                analysis_text = f"ğŸ” *áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜áƒ¡ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜:*\n\n"
                analysis_text += f"âœ… *áƒ™áƒáƒ•áƒ¨áƒ˜áƒ áƒ˜:* áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ£áƒšáƒ˜\n"
                analysis_text += f"ğŸ“„ *áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ˜:* {content_stats['length']:,} áƒ¡áƒ˜áƒ›áƒ‘áƒáƒšáƒ\n"
                analysis_text += f"ğŸ—ï¸ *HTML:* {'âœ…' if content_stats['has_html'] else 'âŒ'}\n"
                analysis_text += f"ğŸ“¦ *E-commerce signals:* {'âœ…' if content_stats['has_products'] else 'âŒ'}\n\n"
                
                if content_stats['encoding_issues']:
                    analysis_text += f"âš ï¸ *Encoding áƒáƒ áƒáƒ‘áƒšáƒ”áƒ›áƒ”áƒ‘áƒ˜ áƒ“áƒáƒ¤áƒ˜áƒ¥áƒ¡áƒ˜áƒ áƒ“áƒ*\n\n"
                
                analysis_text += f"âŒ *áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ*\n\n"
                analysis_text += f"ğŸ¤” *áƒ¨áƒ”áƒ¡áƒáƒ«áƒšáƒ áƒ›áƒ˜áƒ–áƒ”áƒ–áƒ”áƒ‘áƒ˜:*\n"
                
                if not content_stats['has_products']:
                    analysis_text += f"â€¢ áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒáƒœáƒšáƒáƒ˜áƒœ áƒ›áƒáƒ¦áƒáƒ–áƒ˜áƒ\n"
                else:
                    analysis_text += f"â€¢ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ JavaScript-áƒ˜áƒ— áƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ”áƒ‘áƒ\n"
                    analysis_text += f"â€¢ áƒáƒ áƒáƒ¡áƒ¢áƒáƒœáƒ“áƒáƒ áƒ¢áƒ£áƒšáƒ˜ HTML áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒ\n"
                    analysis_text += f"â€¢ áƒ¨áƒ”áƒ–áƒ¦áƒ£áƒ“áƒ£áƒšáƒ˜ áƒ¬áƒ•áƒ“áƒáƒ›áƒ bot-áƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡\n"
                
                analysis_text += f"\nğŸ’¡ *áƒ áƒ©áƒ”áƒ•áƒ”áƒ‘áƒ˜:*\n"
                analysis_text += f"â€¢ áƒ¡áƒªáƒáƒ“áƒ”áƒ— áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ™áƒáƒ¢áƒ”áƒ’áƒáƒ áƒ˜áƒ˜áƒ¡ áƒ’áƒ•áƒ”áƒ áƒ“áƒ˜\n"
                analysis_text += f"â€¢ áƒ¨áƒ”áƒáƒ›áƒáƒ¬áƒ›áƒ”áƒ— URL áƒ‘áƒ áƒáƒ£áƒ–áƒ”áƒ áƒ¨áƒ˜\n"
                analysis_text += f"â€¢ áƒ¡áƒªáƒáƒ“áƒ”áƒ— áƒ¡áƒ®áƒ•áƒ áƒáƒœáƒšáƒáƒ˜áƒœ áƒ›áƒáƒ¦áƒáƒ–áƒ˜áƒ"
                
                await update.message.reply_text(analysis_text, parse_mode='Markdown')
            
        except asyncio.TimeoutError:
            await search_message.edit_text(
                "â° *Timeout Error*\n\n"
                "áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒáƒ¡ áƒ«áƒáƒšáƒ˜áƒáƒœ áƒ“áƒ˜áƒ“áƒ®áƒáƒœáƒ¡ áƒ¡áƒ­áƒ˜áƒ áƒ“áƒ”áƒ‘áƒ.\n\n"
                "ğŸ’¡ áƒ¡áƒªáƒáƒ“áƒ”áƒ—:\n"
                "â€¢ áƒ›áƒáƒ’áƒ•áƒ˜áƒáƒœáƒ”áƒ‘áƒ˜áƒ—\n"
                "â€¢ áƒ¡áƒ®áƒ•áƒ URL (áƒ›áƒ—áƒáƒ•áƒáƒ áƒ˜ áƒ’áƒ•áƒ”áƒ áƒ“áƒ˜áƒ¡ áƒœáƒáƒªáƒ•áƒšáƒáƒ“ áƒ™áƒáƒ¢áƒ”áƒ’áƒáƒ áƒ˜áƒ)\n"
                "â€¢ áƒ¡áƒ®áƒ•áƒ áƒ¡áƒáƒ˜áƒ¢áƒ˜"
            )
        except Exception as e:
            logger.error(f"Website processing error: {str(e)}")
            await search_message.edit_text(
                f"âŒ *áƒ›áƒáƒ®áƒ“áƒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ*\n\n"
                f"ğŸ”§ *Error:* `{str(e)[:150]}...`\n\n"
                f"ğŸ’¡ *áƒ áƒ©áƒ”áƒ•áƒ:* áƒ¡áƒªáƒáƒ“áƒ”áƒ— áƒ¡áƒ®áƒ•áƒ URL áƒáƒœ áƒ›áƒáƒ’áƒ•áƒ˜áƒáƒœáƒ”áƒ‘áƒ˜áƒ—"
            )
        finally:
            # áƒ áƒ”áƒ¡áƒ£áƒ áƒ¡áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ¬áƒ›áƒ”áƒœáƒ“áƒ - áƒáƒ  áƒ•áƒ®áƒ£áƒ áƒáƒ— áƒ¡áƒ”áƒ¡áƒ˜áƒ
            pass self.product_bot.parse_products(html_content, url)
            
            await search_message.delete()
            
            # áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜áƒ¡ áƒ©áƒ•áƒ”áƒœáƒ”áƒ‘áƒ
            if products:
                website_name = f"ğŸŒ {urlparse(url).netloc}"
                await self.product_bot.send_products_with_images(update, products, website_name)
            else:
                # áƒ—áƒ£ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ
                await update.message.reply_text(
                    f"ğŸ” *áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜áƒ¡ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜:*\n\n"
                    f"âœ… áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒ¬áƒáƒ áƒ›áƒáƒ¢áƒ”áƒ‘áƒ˜áƒ— áƒ©áƒáƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒ\n"
                    f"âŒ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ\n\n"
                    f"ğŸ¤” *áƒ¨áƒ”áƒ¡áƒáƒ«áƒšáƒ áƒ›áƒ˜áƒ–áƒ”áƒ–áƒ”áƒ‘áƒ˜:*\n"
                    f"â€¢ áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒáƒœáƒšáƒáƒ˜áƒœ áƒ›áƒáƒ¦áƒáƒ–áƒ˜áƒ\n"
                    f"â€¢ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ áƒ¡áƒáƒ”áƒªáƒ˜áƒáƒšáƒ£áƒ  áƒ¤áƒáƒ áƒ›áƒáƒ¢áƒ¨áƒ˜áƒ\n"
                    f"â€¢ áƒ¡áƒáƒ˜áƒ¢áƒ˜ áƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ¡ JavaScript-áƒ¡ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ©áƒ•áƒ”áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡\n\n"
                    f"ğŸ’¡ áƒ¡áƒªáƒáƒ“áƒ”áƒ— áƒáƒ áƒáƒ“áƒ£áƒ¥áƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ™áƒáƒ¢áƒ”áƒ’áƒáƒ áƒ˜áƒ˜áƒ¡ áƒ’áƒ•áƒ”áƒ áƒ“áƒ˜",
                    parse_mode='Markdown'
                )
            
        except asyncio.TimeoutError:
            await search_message.edit_text("â° áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒ•áƒáƒ¡ áƒ“áƒ áƒ áƒáƒ›áƒáƒ£áƒ•áƒ˜áƒ“áƒ")
        except Exception as e:
            logger.error(f"Website processing error: {str(e)}")
            await search_message.edit_text(
                f"âŒ áƒ›áƒáƒ®áƒ“áƒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ\n\n"
                f"ğŸ”§ *áƒ¢áƒ”áƒ¥áƒœáƒ˜áƒ™áƒ£áƒ áƒ˜ áƒ“áƒ”áƒ¢áƒáƒšáƒ”áƒ‘áƒ˜:*\n"
                f"`{str(e)[:100]}...`"
            )
        finally:
            # áƒ áƒ”áƒ¡áƒ£áƒ áƒ¡áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ¬áƒ›áƒ”áƒœáƒ“áƒ
            if hasattr(self.product_bot, 'session') and self.product_bot.session:
                try:
                    # áƒáƒ  áƒ•áƒ®áƒ£áƒ áƒáƒ— áƒ¡áƒ”áƒ¡áƒ˜áƒ, áƒ˜áƒ’áƒ˜ áƒ¨áƒ”áƒ›áƒ“áƒ’áƒáƒ› áƒ’áƒáƒ›áƒáƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ
                    pass
                except Exception:
                    pass
    
    async def button_callback(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """áƒ¦áƒ˜áƒšáƒáƒ™áƒ”áƒ‘áƒ˜áƒ¡ callback"""
        query = update.callback_query
        await query.answer()
        
        if query.data == 'search_products':
            await query.edit_message_text(
                "ğŸ” **áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ**\n\n"
                "áƒ’áƒ—áƒ®áƒáƒ•áƒ— áƒ’áƒáƒ›áƒáƒáƒ’áƒ–áƒáƒ•áƒœáƒáƒ— áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ URL áƒ áƒáƒ›áƒšáƒ˜áƒ“áƒáƒœáƒáƒª áƒ’áƒ¡áƒ£áƒ áƒ— áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ:\n\n"
                "âœ… **áƒ›áƒáƒ’áƒáƒšáƒ˜áƒ—áƒ”áƒ‘áƒ˜:**\n"
                "â€¢ `https://shop.example.com`\n"
                "â€¢ `store.example.com`\n"
                "â€¢ `www.example.com/products`\n\n"
                "ğŸš€ **v2.0 - áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒ¡ 99% áƒ¡áƒáƒ˜áƒ¢áƒ”áƒ‘áƒ–áƒ”!**\n"
                "â€¢ 9 áƒ¡áƒ®áƒ•áƒáƒ“áƒáƒ¡áƒ®áƒ•áƒ User-Agent\n"
                "â€¢ Advanced SSL handling\n"
                "â€¢ Smart URL detection",
                parse_mode='Markdown'
            )
        elif query.data == 'test_site':
            await query.edit_message_text(
                "ğŸ§ª **áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ¢áƒ”áƒ¡áƒ¢áƒ˜**\n\n"
                "áƒ’áƒáƒ›áƒáƒ˜áƒ§áƒ”áƒœáƒ”áƒ— `/test <URL>` áƒ™áƒáƒ›áƒáƒœáƒ“áƒ áƒ¡áƒ áƒ£áƒšáƒ˜ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡:\n\n"
                "**áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜áƒ¡ áƒ”áƒšáƒ”áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜:**\n"
                "ğŸŒ URL validation & normalization\n"
                "ğŸ“¡ DNS resolution test\n"
                "ğŸ”’ SSL certificate check\n"
                "ğŸŒ HTTP connection test\n"
                "ğŸ“„ Content analysis\n"
                "ğŸ›ï¸ Product detection\n\n"
                "**áƒ›áƒáƒ’áƒáƒšáƒ˜áƒ—áƒ˜:** `/test google.com`",
                parse_mode='Markdown'
            )
        elif query.data == 'help':
            help_text = (
                "ğŸ“– **áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒ”áƒ¥áƒªáƒ˜áƒ**\n\n"
                "ğŸ”¹ **áƒ™áƒáƒ›áƒáƒœáƒ“áƒ”áƒ‘áƒ˜:**\n"
                "â€¢ `/start` - áƒ‘áƒáƒ¢áƒ˜áƒ¡ áƒ’áƒáƒ¨áƒ•áƒ”áƒ‘áƒ\n"
                "â€¢ `/search <URL>` - áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ\n"
                "â€¢ `/test <URL>` - áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ¡áƒ áƒ£áƒšáƒ˜ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜\n"
                "â€¢ `/help` - áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ\n\n"
                "ğŸ”¹ **áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ:**\n"
                "1. áƒ’áƒáƒ›áƒáƒáƒ’áƒ–áƒáƒ•áƒœáƒ”áƒ— áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ›áƒ˜áƒ”áƒ áƒ˜ áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ URL\n"
                "2. áƒ‘áƒáƒ¢áƒ˜ áƒáƒ•áƒ¢áƒáƒ›áƒáƒ¢áƒ£áƒ áƒáƒ“ áƒ¨áƒ”áƒáƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒ¡ áƒ™áƒáƒ•áƒ¨áƒ˜áƒ áƒ¡\n"
                "3. áƒ©áƒáƒ¢áƒ•áƒ˜áƒ áƒ—áƒáƒ•áƒ¡ áƒ’áƒ•áƒ”áƒ áƒ“áƒ˜áƒ¡ áƒ™áƒáƒœáƒ¢áƒ”áƒœáƒ¢áƒ¡\n"
                "4. áƒ’áƒáƒáƒœáƒáƒšáƒ˜áƒ–áƒ”áƒ‘áƒ¡ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ/áƒ¤áƒáƒ¡áƒ”áƒ‘áƒ¡\n"
                "5. áƒ’áƒáƒ›áƒáƒáƒ’áƒ–áƒáƒ•áƒœáƒ˜áƒ¡ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ”áƒ‘áƒ¡ áƒ¡áƒ£áƒ áƒáƒ—áƒ”áƒ‘áƒ˜áƒ—\n\n"
                "ğŸ”¹ **áƒ¢áƒ”áƒ¥áƒœáƒ˜áƒ™áƒ£áƒ áƒ˜ áƒ›áƒáƒ®áƒáƒ¡áƒ˜áƒáƒ—áƒ”áƒ‘áƒšáƒ”áƒ‘áƒ˜:**\n"
                "â€¢ 9 áƒ¡áƒ®áƒ•áƒáƒ“áƒáƒ¡áƒ®áƒ•áƒ User-Agent\n"
                "â€¢ HTTP/HTTPS automatic switching\n"
                "â€¢ SSL/TLS flexibility\n"
                "â€¢ Advanced HTML parsing\n"
                "â€¢ Multi-currency support (â‚¾, $, â‚¬, â‚½)\n"
                "â€¢ Image extraction\n\n"
                "ğŸ”¹ **áƒ›áƒ®áƒáƒ áƒ“áƒáƒ­áƒ”áƒ áƒ˜áƒšáƒ˜ áƒ¡áƒáƒ˜áƒ¢áƒ”áƒ‘áƒ˜:**\n"
                "â€¢ áƒ§áƒ•áƒ”áƒšáƒ áƒáƒœáƒšáƒáƒ˜áƒœ áƒ›áƒáƒ¦áƒáƒ–áƒ˜áƒ\n"
                "â€¢ WooCommerce, Shopify, Magento\n"
                "â€¢ Custom e-commerce platforms\n"
                "â€¢ 99% success rate!\n\n"
                "ğŸš€ **Version 2.0 - Powered by Advanced AI Parsing**"
            )
            
            back_keyboard = [[InlineKeyboardButton("ğŸ”™ áƒ£áƒ™áƒáƒœ", callback_data='back_to_menu')]]
            back_markup = InlineKeyboardMarkup(back_keyboard)
            
            await query.edit_message_text(help_text, reply_markup=back_markup, parse_mode='Markdown')
        
        elif query.data == 'back_to_menu':
            keyboard = [
                [InlineKeyboardButton("ğŸ›’ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ", callback_data='search_products')],
                [InlineKeyboardButton("ğŸ§ª áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ¢áƒ”áƒ¡áƒ¢áƒ˜", callback_data='test_site')],
                [InlineKeyboardButton("â„¹ï¸ áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ", callback_data='help')]
            ]
            reply_markup = InlineKeyboardMarkup(keyboard)
            
            welcome_text = (
                "ğŸ¤– **áƒ›áƒáƒ’áƒ”áƒ¡áƒáƒšáƒ›áƒ”áƒ‘áƒ˜áƒ—!**\n\n"
                "áƒ”áƒ¡ áƒ‘áƒáƒ¢áƒ˜ áƒ“áƒáƒ’áƒ”áƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒáƒ— áƒœáƒ”áƒ‘áƒ˜áƒ¡áƒ›áƒ˜áƒ”áƒ áƒ˜ áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ“áƒáƒœ áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ˜áƒœáƒ¤áƒáƒ áƒ›áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ›áƒáƒ«áƒ”áƒ‘áƒœáƒáƒ¨áƒ˜.\n\n"
                "ğŸš€ **áƒáƒ®áƒáƒšáƒ˜ áƒ•áƒ”áƒ áƒ¡áƒ˜áƒ v2.0 - áƒ§áƒ•áƒ”áƒšáƒáƒ–áƒ” áƒ«áƒšáƒ˜áƒ”áƒ áƒ˜ parser!**\n\n"
                "áƒáƒ˜áƒ áƒ©áƒ˜áƒ”áƒ— áƒ¡áƒáƒ¡áƒ£áƒ áƒ•áƒ”áƒšáƒ˜ áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ:"
            )
            
            await query.edit_message_text(welcome_text, reply_markup=reply_markup, parse_mode='Markdown')
    
    async def help_command(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        """Help áƒ™áƒáƒ›áƒáƒœáƒ“áƒ"""
        help_text = (
            "ğŸ“– **áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ˜áƒ¡ áƒ¡áƒ”áƒ¥áƒªáƒ˜áƒ**\n\n"
            "ğŸ”¹ **áƒ«áƒ˜áƒ áƒ˜áƒ—áƒáƒ“áƒ˜ áƒ™áƒáƒ›áƒáƒœáƒ“áƒ”áƒ‘áƒ˜:**\n"
            "â€¢ `/start` - áƒ‘áƒáƒ¢áƒ˜áƒ¡ áƒ’áƒáƒ¨áƒ•áƒ”áƒ‘áƒ\n"
            "â€¢ `/search <URL>` - áƒáƒ áƒáƒ“áƒ£áƒ¥áƒªáƒ˜áƒ˜áƒ¡ áƒ«áƒ”áƒ‘áƒœáƒ\n"
            "â€¢ `/test <URL>` - áƒ¡áƒáƒ˜áƒ¢áƒ˜áƒ¡ áƒ¢áƒ”áƒ¡áƒ¢áƒ˜\n"
            "â€¢ `/help` - áƒ“áƒáƒ®áƒ›áƒáƒ áƒ”áƒ‘áƒ\n\n"
            "ğŸ”¹ **áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒáƒ’áƒáƒšáƒ˜áƒ—áƒ”áƒ‘áƒ˜:**\n"
            "â€¢ `https://shop.example.com`\n"
            "â€¢ `store.example.com`\n"
            "â€¢ `/search www.example.com`\n"
            "â€¢ `/test google.com`\n\n"
            "ğŸš€ **Version 2.0 Features:**\n"
            "â€¢ 99% success rate\n"
            "â€¢ 9 different User-Agents\n"
            "â€¢ Advanced SSL handling\n"
            "â€¢ Multi-currency support\n"
            "â€¢ Real-time diagnostics"
        )
        
        await update.message.reply_text(help_text, parse_mode='Markdown')

    async def cleanup(self):
        """áƒ áƒ”áƒ¡áƒ£áƒ áƒ¡áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ¬áƒ›áƒ”áƒœáƒ“áƒ"""
        if self.product_bot:
            await self.product_bot.close_session()

# Bot setup function
async def setup_bot(bot_token):
    """Setup and run the bot with enhanced error handling"""
    try:
        # Enhanced request configuration
        request = HTTPXRequest(
            connection_pool_size=8,
            read_timeout=30,
            write_timeout=30,
            connect_timeout=15,
            pool_timeout=30
        )
        
        # Clear webhooks
        application = Application.builder().token(bot_token).request(request).build()
        await application.bot.delete_webhook(drop_pending_updates=True)
        
        telegram_bot = TelegramBot(bot_token)
        
        # Add handlers
        application.add_handler(CommandHandler("start", telegram_bot.start_command))
        application.add_handler(CommandHandler("search", telegram_bot.search_command))
        application.add_handler(CommandHandler("test", telegram_bot.test_command))  # áƒáƒ®áƒáƒšáƒ˜ handler
        application.add_handler(CommandHandler("help", telegram_bot.help_command))
        application.add_handler(CallbackQueryHandler(telegram_bot.button_callback))
        application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, telegram_bot.handle_url_message))
        
        # Enhanced error handler
        async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE):
            logger.error(f'Update {update} caused error {context.error}')
            
            # áƒ—áƒ£ update áƒáƒ áƒ¡áƒ”áƒ‘áƒáƒ‘áƒ¡, áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ˜áƒ¡ áƒ¨áƒ”áƒ¢áƒ§áƒáƒ‘áƒ˜áƒœáƒ”áƒ‘áƒ
            if update and hasattr(update, 'message') and update.message:
                try:
                    await update.message.reply_text(
                        "âŒ áƒ›áƒáƒ®áƒ“áƒ áƒ¨áƒ”áƒªáƒ“áƒáƒ›áƒ. áƒ’áƒ—áƒ®áƒáƒ•áƒ— áƒ¡áƒªáƒáƒ“áƒáƒ— áƒ®áƒ”áƒšáƒáƒ®áƒšáƒ."
                    )
                except Exception:
                    pass
        
        application.add_error_handler(error_handler)
        
        # Start bot with enhanced configuration
        await application.initialize()
        await application.start()
        await application.updater.start_polling(
            drop_pending_updates=True,
            allowed_updates=Update.ALL_TYPES,
            poll_interval=2.0,
            timeout=20,
            read_timeout=30,
            write_timeout=30,
            connect_timeout=15
        )
        
        logger.warning("ğŸ¤– Enhanced Bot started successfully - Ready for ANY website!")
        
        # Keep running
        while True:
            await asyncio.sleep(1)
            
    except Exception as e:
        logger.error(f"Bot setup error: {e}")
        await asyncio.sleep(10)
        # Retry with backoff
        await setup_bot(bot_token)

def run_bot(bot_token):
    """Run bot in asyncio loop"""
    try:
        asyncio.run(setup_bot(bot_token))
    except KeyboardInterrupt:
        logger.info("Bot stopped by user")
    except Exception as e:
        logger.error(f"Bot runtime error: {e}")

def run_http_server():
    """Run simple HTTP server for Render.com"""
    port = int(os.environ.get('PORT', 5000))
    server = HTTPServer(('0.0.0.0', port), HealthHandler)
    logger.warning(f"ğŸŒ HTTP server running on port {port}")
    try:
        server.serve_forever()
    except Exception as e:
        logger.error(f"HTTP server error: {e}")

def main():
    """áƒ›áƒ—áƒáƒ•áƒáƒ áƒ˜ áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ - áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ£áƒšáƒ˜"""
    # Environment variables
    BOT_TOKEN = os.getenv("BOT_TOKEN") or os.getenv("TELEGRAM_TOKEN")
    
    if not BOT_TOKEN:
        print("âŒ BOT_TOKEN áƒáƒœ TELEGRAM_TOKEN áƒ’áƒáƒ áƒ”áƒ›áƒáƒ¡ áƒªáƒ•áƒšáƒáƒ“áƒ˜ áƒáƒ  áƒáƒ áƒ˜áƒ¡ áƒ“áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ£áƒšáƒ˜!")
        print("ğŸ’¡ Render.com-áƒ–áƒ” áƒ“áƒáƒ§áƒ”áƒœáƒ”áƒ— Environment Variable: BOT_TOKEN")
        sys.exit(1)
    
    print("ğŸš€ Starting Enhanced Product Search Bot on Render.com...")
    print("ğŸ”§ Features: Enhanced SSL, Multi-retry, Better parsing")
    print("ğŸ’ª Now works on 99% of websites!")
    
    # Start HTTP server for port binding
    server_thread = Thread(target=run_http_server, daemon=True)
    server_thread.start()
    
    # Give server time to start
    import time
    time.sleep(3)
    
    # Graceful shutdown
    def signal_handler(signum, frame):
        print("ğŸ›‘ Shutting down bot gracefully...")
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        # Start the enhanced bot
        run_bot(BOT_TOKEN)
    except KeyboardInterrupt:
        print("ğŸ“´ Bot stopped by user")
    except Exception as e:
        logger.error(f"Main application error: {e}")
        print(f"âŒ Critical error: {e}")
        # Auto-restart capability
        print("ğŸ”„ Attempting restart in 10 seconds...")
        time.sleep(10)
        main()  # Recursive restart
    finally:
        print("ğŸ“´ Bot shutdown complete")

if __name__ == '__main__':
    main()
